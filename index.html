<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="我的技美学习之路">
<meta property="og:type" content="website">
<meta property="og:title" content="ChrisZhang">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="ChrisZhang">
<meta property="og:description" content="我的技美学习之路">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="ChrisZhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>ChrisZhang</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">ChrisZhang</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-download fa-fw"></i>资源</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/03/Tensowflow%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="ChrisZhang">
      <meta itemprop="description" content="我的技美学习之路">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ChrisZhang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/03/Tensowflow%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0-2/" class="post-title-link" itemprop="url">Tensowflow基础学习(2)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-05-03 12:26:48 / 修改时间：12:32:09" itemprop="dateCreated datePublished" datetime="2022-05-03T12:26:48+08:00">2022-05-03</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/tensorflow/" itemprop="url" rel="index"><span itemprop="name">tensorflow</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Tensorflow学习-2"><a href="#Tensorflow学习-2" class="headerlink" title="Tensorflow学习(2)"></a>Tensorflow学习(2)</h2><p>在这篇博客当中,将会整理如下内容:</p>
<ul>
<li>预备知识</li>
<li>神经网络复杂度</li>
<li>指数衰减学习率</li>
<li>激活函数</li>
<li>损失函数</li>
<li>欠拟合和过拟合</li>
<li>正则化减少过拟合</li>
<li>优化器更新网络参数</li>
</ul>
<h2 id="2-1-预备知识"><a href="#2-1-预备知识" class="headerlink" title="2.1 预备知识"></a>2.1 预备知识</h2><h3 id="1-tf-where函数"><a href="#1-tf-where函数" class="headerlink" title="(1)tf.where函数"></a>(1)tf.where函数</h3><p><code>tf.where(条件语句,真返回A,假返回B)</code></p>
<p>比如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">b = tf.constant([<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">c = tf.where(tf.greater(a, b), a, b)  <span class="comment"># 如果a&gt;b,则返回a对应的元素,否则返回b(对应元素位置比较)</span></span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行结果:</span></span><br><span class="line">tf.Tensor([<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span>], shape=(<span class="number">5</span>,), dtype=int32)</span><br></pre></td></tr></table></figure>



<h3 id="2-np-random-RandomState-rand"><a href="#2-np-random-RandomState-rand" class="headerlink" title="(2)np.random.RandomState.rand()"></a>(2)np.random.RandomState.rand()</h3><p>返回一个**[0,1)(注意是左闭右开区间)**之间的随机数.</p>
<p><code>np.random.RandomState.rand(维度)</code></p>
<p>如果维度为空,则返回标量.</p>
<p>比如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(seed=<span class="number">1</span>)  <span class="comment"># seed=1,种子一样,不同的机器跑出的随机数应该一致</span></span><br><span class="line">a = rdm.rand()  <span class="comment"># 返回一个随机标量</span></span><br><span class="line">b = rdm.rand(<span class="number">2</span>, <span class="number">3</span>)  <span class="comment"># 返回维度2行3列的随机数矩阵</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;a:&quot;</span>, a)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b:&quot;</span>, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">a: <span class="number">0.417022004702574</span></span><br><span class="line">b: [[<span class="number">7.20324493e-01</span> <span class="number">1.14374817e-04</span> <span class="number">3.02332573e-01</span>]</span><br><span class="line"> [<span class="number">1.46755891e-01</span> <span class="number">9.23385948e-02</span> <span class="number">1.86260211e-01</span>]]</span><br></pre></td></tr></table></figure>



<h3 id="3-np-vstack"><a href="#3-np-vstack" class="headerlink" title="(3)np.vstack()"></a>(3)np.vstack()</h3><p>可以将两个数组按垂直方向叠加</p>
<p><code>np.vstack(数组1,数组2)</code></p>
<p>比如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = np.array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">c = np.vstack((a, b))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;c:\n&quot;</span>, c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果:</span></span><br><span class="line">c:</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line">[<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br></pre></td></tr></table></figure>



<h3 id="4-np-mgrid-ravel-和np-c"><a href="#4-np-mgrid-ravel-和np-c" class="headerlink" title="(4)np.mgrid[],.ravel()和np.c_[]"></a>(4)<code>np.mgrid[],.ravel()和np.c_[]</code></h3><p>经常一起使用,<strong>用来生成网格坐标点</strong>.</p>
<p>(a)<code>np.mgrid[起始值:结束值:步长,起始值:结束值:步长,...]</code></p>
<p>这里的起始值和结束值是左闭右开区间(<strong>[起始值,结束值)</strong>)</p>
<p>(b)<code>x.ravel()</code></p>
<p>将x变为一维数组,可以理解成”把<strong>·</strong>前的变量拉直”(也就是x)</p>
<p>(c)<code>np.c_[]</code></p>
<p>使得返回的间隔数值点配对,比如<code>np.c_[数组1,数组2,...]</code></p>
<p>比如说:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成等间隔数值点</span></span><br><span class="line">x, y = np.mgrid[<span class="number">1</span>:<span class="number">3</span>:<span class="number">1</span>, <span class="number">2</span>:<span class="number">4</span>:<span class="number">0.5</span>]</span><br><span class="line"><span class="comment"># 将x, y拉直，并合并配对为二维张量，生成二维坐标点</span></span><br><span class="line">grid = np.c_[x.ravel(), y.ravel()]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x:\n&quot;</span>, x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y:\n&quot;</span>, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x.ravel():\n&quot;</span>, x.ravel())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y.ravel():\n&quot;</span>, y.ravel())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;grid:\n&#x27;</span>, grid)</span><br></pre></td></tr></table></figure>

<p>输出结果如下:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502194342359.png" alt="image-20220502194342359"></p>
<h2 id="2-2-复杂度与学习率"><a href="#2-2-复杂度与学习率" class="headerlink" title="2.2 复杂度与学习率"></a>2.2 复杂度与学习率</h2><h3 id="1-复杂度"><a href="#1-复杂度" class="headerlink" title="(1)复杂度"></a>(1)复杂度</h3><p>以下图为例:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502194551898.png" alt="image-20220502194551898"></p>
<p>空间复杂度:</p>
<ul>
<li><p>包含层数&#x3D;隐藏层的层数+1个输出层(<strong>因为输入层不具备运算能力,因此不算空间复杂度</strong>)</p>
</li>
<li><p>总参数&#x3D;总w+总b</p>
<ul>
<li>对应上图,总参数&#x3D;3×4+4(第一层)+4×2+2(第二层)&#x3D;26</li>
<li>每个神经元有一个偏置值b</li>
</ul>
</li>
</ul>
<p>时间复杂度:</p>
<ul>
<li>乘加运算次数<ul>
<li>对应上图:3×4+4×2&#x3D;20</li>
<li><strong>直观理解,有几条神经线就有几次乘加运算</strong></li>
</ul>
</li>
</ul>
<h3 id="2-学习率"><a href="#2-学习率" class="headerlink" title="(2)学习率"></a>(2)学习率</h3><p>回顾之前内容,学习率不应该设置的过大或者过小.</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502194945970.png" alt="image-20220502194945970"></p>
<ul>
<li>实际应用中,可以先用较大的学习率,快速得到最优解,然后逐步减小学习率,使模型在训练后期更加稳定.</li>
</ul>
<h4 id="指数衰减学习率"><a href="#指数衰减学习率" class="headerlink" title="指数衰减学习率"></a>指数衰减学习率</h4><p>指数衰减学习率可以表示为:<br>$$<br>初始学习率×学习率衰减率^{(当前轮数&#x2F;多少轮衰减一次)}<br>$$<br>比如下述的代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w = tf.Variable(tf.constant(<span class="number">5</span>, dtype=tf.float32))</span><br><span class="line"></span><br><span class="line">epoch = <span class="number">40</span></span><br><span class="line">LR_BASE = <span class="number">0.2</span>  <span class="comment"># 最初学习率</span></span><br><span class="line">LR_DECAY = <span class="number">0.99</span>  <span class="comment"># 学习率衰减率</span></span><br><span class="line">LR_STEP = <span class="number">1</span>  <span class="comment"># 喂入多少轮BATCH_SIZE后，更新一次学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):  <span class="comment"># for epoch 定义顶层循环，表示对数据集循环epoch次，此例数据集数据仅有1个w,初始化时候constant赋值为5，循环100次迭代。</span></span><br><span class="line">    lr = LR_BASE * LR_DECAY ** (epoch / LR_STEP)</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># with结构到grads框起了梯度的计算过程。</span></span><br><span class="line">        loss = tf.square(w + <span class="number">1</span>)</span><br><span class="line">    grads = tape.gradient(loss, w)  <span class="comment"># .gradient函数告知谁对谁求导</span></span><br><span class="line"></span><br><span class="line">    w.assign_sub(lr * grads)  <span class="comment"># .assign_sub 对变量做自减 即：w -= lr*grads 即 w = w - lr*grads</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;After %s epoch,w is %f,loss is %f,lr is %f&quot;</span> % (epoch, w.numpy(), loss, lr))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>核心是<code>lr = LR_BASE * LR_DECAY ** (epoch / LR_STEP)</code>一句,符合上述的公式.</p>
<p>此时运行查看结果,可以看到学习率lr呈现指数衰减:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502195548124.png" alt="image-20220502195548124"></p>
<h2 id="2-3-激活函数"><a href="#2-3-激活函数" class="headerlink" title="2.3 激活函数"></a>2.3 激活函数</h2><h3 id="1-sigmoid函数"><a href="#1-sigmoid函数" class="headerlink" title="(1)sigmoid函数"></a>(1)<code>sigmoid</code>函数</h3><p><code>tf.nn.sigmoid(x)</code><br>$$<br>f(x)&#x3D;\frac{1}{1+e^{-x}}<br>$$<br><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502212022490.png" alt="image-20220502212022490"></p>
<p>近年来,使用sigmoid作为激活函数的神经网络已经不多了,<strong>原因是链式求导会导致部分导数越来越小(从右图可以看到导数范围0-0.25,可能会越乘越小),从而使梯度消失</strong></p>
<h3 id="2-Tanh函数"><a href="#2-Tanh函数" class="headerlink" title="(2)Tanh函数"></a>(2)<code>Tanh</code>函数</h3><p><code>tf.math.tanh(x)</code></p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502212359530.png" alt="image-20220502212359530"></p>
<h3 id="3-Relu函数"><a href="#3-Relu函数" class="headerlink" title="(3)Relu函数"></a>(3)<code>Relu</code>函数</h3><p><code>tf.nn.relu(x)</code></p>
<p>这是一个分段函数,方程如下:<br>$$<br>f(x)&#x3D;max(x,0)&#x3D;\begin{cases}0&amp;x&lt;0 \\<br>x&amp;x&gt;&#x3D;0\end{cases}<br>$$<br><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502212618766.png" alt="image-20220502212618766"></p>
<p>补充:导致神经元死亡的根本原因是经过Relu函数的负特征过多,可以通过设置更小的学习率,减少参数分布的巨大变化,避免训练中产生过多负特征进入Relu.</p>
<h3 id="4-Leaky-Relu函数"><a href="#4-Leaky-Relu函数" class="headerlink" title="(4)Leaky Relu函数"></a>(4)<code>Leaky Relu</code>函数</h3><p>为了解决Relu负参数所引发的神经元死亡问题.虽然比Relu更好,但实际使用中使用Relu作为激活函数的网络更多.</p>
<p><code>tf.nn.leaky_relu(x)</code></p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502212948542.png" alt="image-20220502212948542"></p>
<h3 id="5-对于初学者的建议"><a href="#5-对于初学者的建议" class="headerlink" title="(5)对于初学者的建议"></a>(5)对于初学者的建议</h3><ul>
<li>首选Relu激活函数</li>
<li>学习率设置较小值</li>
<li>输入特征标准化,即让输入特征满足以0为均值,1为标准差的正态分布</li>
<li>初始参数中心化,即让随机生成的参数满足以0为均值,$\large\sqrt{\frac{2}{当前层输入特征个数}}$为标准差的正态分布.</li>
</ul>
<h2 id="2-4-损失函数loss"><a href="#2-4-损失函数loss" class="headerlink" title="2.4 损失函数loss"></a>2.4 损失函数loss</h2><p>定义:预测值(y)与已知答案(y_)之间的差距.</p>
<p>主流loss有三种计算方法:</p>
<ul>
<li><strong>mse</strong>(Mean Squared Error,均方误差)</li>
<li>自定义</li>
<li><strong>ce</strong>(Cross Entropy)</li>
</ul>
<h3 id="1-均方误差"><a href="#1-均方误差" class="headerlink" title="(1)均方误差"></a>(1)均方误差</h3><p>$$<br>MES(y_,y)&#x3D;\large\frac{\sum_{i&#x3D;1}^N(y-y_)^2}{n}<br>$$</p>
<p><code>loss_mse=tf.reduce_mean(tf.square(y_-y))</code></p>
<p>引入一个例子:预测酸奶日销量(<strong>噪声设置是为了引入一定的随机值误差</strong>):</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502213813476.png" alt="image-20220502213813476"></p>
<p>相关代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">23455</span></span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(seed=SEED)  <span class="comment"># 生成[0,1)之间的随机数</span></span><br><span class="line">x = rdm.rand(<span class="number">32</span>, <span class="number">2</span>)  <span class="comment"># 生成的输入特征</span></span><br><span class="line">y_ = [[x1 + x2 + (rdm.rand() / <span class="number">10.0</span> - <span class="number">0.05</span>)] <span class="keyword">for</span> (x1, x2) <span class="keyword">in</span> x]  <span class="comment"># 生成噪声[0,1)/10=[0,0.1);构建标准答案 [0,0.1)-0.05=[-0.05,0.05)</span></span><br><span class="line">x = tf.cast(x, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">epoch = <span class="number">15000</span></span><br><span class="line">lr = <span class="number">0.002</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        y = tf.matmul(x, w1)</span><br><span class="line">        loss_mse = tf.reduce_mean(tf.square(y_ - y))</span><br><span class="line"></span><br><span class="line">    grads = tape.gradient(loss_mse, w1)</span><br><span class="line">    w1.assign_sub(lr * grads)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;After %d training steps,w1 is &quot;</span> % (epoch))</span><br><span class="line">        <span class="built_in">print</span>(w1.numpy(), <span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Final w1 is: &quot;</span>, w1.numpy())</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>最后发现,神经网络拟合出的结果与前面设置的保持一致,说明网络确实可以起到训练的效果.</p>
<p>输出结果的最后部分如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">After 14500 training steps,w1 is </span><br><span class="line">[[1.0002553 ]</span><br><span class="line"> [0.99838644]] </span><br><span class="line"></span><br><span class="line">Final w1 is:  [[1.0009792]</span><br><span class="line"> [0.9977485]]</span><br></pre></td></tr></table></figure>



<h3 id="2-自定义损失函数"><a href="#2-自定义损失函数" class="headerlink" title="(2)自定义损失函数"></a>(2)自定义损失函数</h3><p>上面的案例中,我们都使用均方误差来作为损失函数的参考依据,<strong>但这实际上是不准确的</strong>.因为当预测商品销量多了的话,损失的会是成本;而当预测商品销量少了的话,损失的会是利润,<strong>因此可以使用自定义的函数来处理</strong>(因为实际上成本和利润大概率是不相等的).</p>
<p>比如如下的损失函数是可以的:<br>$$<br>loss(y_,y)&#x3D;\sum_{n}f(y_,y) \<br>其中\sum_{n}f(y_,y)&#x3D;\begin{cases}PROFIT*(y_-y)&amp; y&lt;y_(预测的y少了,损失利润) \<br>COST*(y-y_)&amp; y&gt;&#x3D;y_(预测的y多了,损失成本) \end{cases}<br>$$<br>此时损失函数可以这样写:</p>
<p><code>loss_zdy=tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_) * COST, (y_ - y) * PROFIT))</code></p>
<p>举一个极端情况:</p>
<blockquote>
<p>酸奶成本1元， 酸奶利润99元<br>成本很低，利润很高，显然预测少了带来的损失更为严重,而人们希望多预测些，生成模型系数大于1，往多了进行预测.</p>
</blockquote>
<p>将损失函数替换为上述代码,得到的结果如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">After 14500 training steps,w1 is </span><br><span class="line">[[1.1406062]</span><br><span class="line"> [1.0476325]] </span><br><span class="line"></span><br><span class="line">Final w1 is:  [[1.1420636]</span><br><span class="line"> [1.1016785]]</span><br></pre></td></tr></table></figure>

<p>可以看到,训练结果会尽可能让值大于初始的设定值,来保障利润.</p>
<p>如果酸奶成本99元,利润1元,则会使得训练的参数结果&lt;1(尽可能保持成本)</p>
<h3 id="3-交叉熵"><a href="#3-交叉熵" class="headerlink" title="(3)交叉熵"></a>(3)交叉熵</h3><p><code>tf.losses.categorical_crossentropy(y_,y)</code></p>
<p>交叉熵损失函数CE:表征两个概率分布之间的距离</p>
<p><strong>交叉熵越大,两个概率分布越远;</strong><br>$$<br>H(y_,y)&#x3D;\sum y_*lny<br>$$<br>比如二分类问题,可以很好地验证哪个分类结果更加接近标准答案:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502222420738.png" alt="image-20220502222420738"></p>
<p>通过定量计算,发现第二种结果的交叉熵更小,因此两个概率分布更为接近一些,所以y2预测的更准.</p>
<p>上述验证的代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">loss_ce1 = tf.losses.categorical_crossentropy([<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0.6</span>, <span class="number">0.4</span>])</span><br><span class="line">loss_ce2 = tf.losses.categorical_crossentropy([<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0.8</span>, <span class="number">0.2</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;loss_ce1:&quot;</span>, loss_ce1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;loss_ce2:&quot;</span>, loss_ce2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉熵损失函数</span></span><br></pre></td></tr></table></figure>



<h3 id="4-softmax与交叉熵结合"><a href="#4-softmax与交叉熵结合" class="headerlink" title="(4)softmax与交叉熵结合"></a>(4)softmax与交叉熵结合</h3><ul>
<li>输出先过softmax函数,再计算y与y_的交叉熵损失函数.<ul>
<li>回顾softmax函数:使输出的值符合概率的分布(用于将网络输出值进行处理,使其符合概率的分布)</li>
</ul>
</li>
</ul>
<p><code>tf.nn.softmax_cross_entropy_with_logits(y_,y)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># softmax与交叉熵损失函数的结合</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">y_ = np.array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">y = np.array([[<span class="number">12</span>, <span class="number">3</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">10</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>], [<span class="number">4</span>, <span class="number">6.5</span>, <span class="number">1.2</span>], [<span class="number">3</span>, <span class="number">6</span>, <span class="number">1</span>]])</span><br><span class="line">y_pro = tf.nn.softmax(y)</span><br><span class="line">loss_ce1 = tf.losses.categorical_crossentropy(y_,y_pro)</span><br><span class="line"><span class="comment"># 以上两句可以等同于下面的这一句(softmax与交叉熵结合)</span></span><br><span class="line">loss_ce2 = tf.nn.softmax_cross_entropy_with_logits(y_, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;分步计算的结果:\n&#x27;</span>, loss_ce1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;结合计算的结果:\n&#x27;</span>, loss_ce2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出的结果相同</span></span><br></pre></td></tr></table></figure>



<h2 id="2-5-缓解过拟合与欠拟合"><a href="#2-5-缓解过拟合与欠拟合" class="headerlink" title="2.5 缓解过拟合与欠拟合"></a>2.5 缓解过拟合与欠拟合</h2><p>欠拟合的解决方法:</p>
<ul>
<li>增加输入特征项</li>
<li>增加网络参数</li>
<li>减少正则化参数</li>
</ul>
<p>过拟合的解决方法:</p>
<ul>
<li>数据清洗:比如减少噪声,使数据集更纯净等</li>
<li>增大训练集</li>
<li>采用正则化</li>
<li>增大正则化参数</li>
</ul>
<h3 id="1-正则化缓解过拟合"><a href="#1-正则化缓解过拟合" class="headerlink" title="(1)正则化缓解过拟合"></a>(1)正则化缓解过拟合</h3><p><strong>正则化在损失函数中引入模型复杂度指标,利用给W加权值,弱化了训练数据的噪声(一般不正则化b)</strong><br>$$<br>loss&#x3D;loss(y,y_)+REGULARIZER*loss(w)<br>$$<br>其中,前一项是模型中所有参数的损失函数,比如交叉熵,均方误差等.</p>
<p><strong>RRGULARIZER是超参数,给出参数w在总loss中的比例,即正则化的权重</strong></p>
<p>w是需要正则化的参数</p>
<p>一般正则化分为L1正则化和L2正则化:</p>
<ul>
<li>L1正则化</li>
</ul>
<p>$$<br>loss_{L1}(w)&#x3D;\sum_i |w_i|<br>$$</p>
<ul>
<li>L2正则化</li>
</ul>
<p>$$<br>loss_{L2}(w)&#x3D;\sum_i |w_i^2|<br>$$</p>
<p>对比:</p>
<p>L1正则化大概率会使很多参数变为0,<strong>因此该方法可通过稀疏参数</strong>,即减少参数的数量,来降低复杂度.</p>
<p>L2正则化会使得参数很接近零但不为零,<strong>因此该方法可通过减少参数值的大小降低复杂度</strong>.</p>
<p>以下以L2正则化为例,其核心代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span></span><br><span class="line">loss_mse = tf.reduce_mean(tf.square(y_train - y))</span><br><span class="line"><span class="comment"># 添加l2正则化</span></span><br><span class="line">loss_regularization = []</span><br><span class="line"><span class="comment"># tf.nn.l2_loss(w)=sum(w ** 2) / 2</span></span><br><span class="line">loss_regularization.append(tf.nn.l2_loss(w1))</span><br><span class="line">oss_regularization.append(tf.nn.l2_loss(w2))</span><br></pre></td></tr></table></figure>



<p>举例:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220503110955117.png" alt="image-20220503110955117"></p>
<p>通过模拟网格,将数据放置在网格上,<strong>将网格坐标点作为输入值送入神经网络当中</strong>,神经网络会为每个坐标输出0&#x2F;1,可以将输出值为0.5的点连线,作为两个分类的分割线.(上图中红色为标签1,蓝色为标签0)</p>
<p>实现代码如下(<strong>注意,要引入课件当中的csv文件作为读取数据</strong>):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需模块</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入数据/标签 生成x_train y_train</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;dot.csv&#x27;</span>)</span><br><span class="line">x_data = np.array(df[[<span class="string">&#x27;x1&#x27;</span>, <span class="string">&#x27;x2&#x27;</span>]])</span><br><span class="line">y_data = np.array(df[<span class="string">&#x27;y_c&#x27;</span>])</span><br><span class="line"></span><br><span class="line">x_train = np.vstack(x_data).reshape(-<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">y_train = np.vstack(y_data).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">Y_c = [[<span class="string">&#x27;red&#x27;</span> <span class="keyword">if</span> y <span class="keyword">else</span> <span class="string">&#x27;blue&#x27;</span>] <span class="keyword">for</span> y <span class="keyword">in</span> y_train]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换x的数据类型，否则后面矩阵相乘时会因数据类型问题报错</span></span><br><span class="line">x_train = tf.cast(x_train, tf.float32)</span><br><span class="line">y_train = tf.cast(y_train, tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># from_tensor_slices函数切分传入的张量的第一个维度，生成相应的数据集，使输入特征和标签值一一对应</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成神经网络的参数，输入层为2个神经元，隐藏层为11个神经元(这个数是随便选的,可以根据情况进行修改)，1层隐藏层，输出层为1个神经元</span></span><br><span class="line"><span class="comment"># 用tf.Variable()保证参数可训练</span></span><br><span class="line">w1 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">11</span>]), dtype=tf.float32)</span><br><span class="line">b1 = tf.Variable(tf.constant(<span class="number">0.01</span>, shape=[<span class="number">11</span>]))</span><br><span class="line"></span><br><span class="line">w2 = tf.Variable(tf.random.normal([<span class="number">11</span>, <span class="number">1</span>]), dtype=tf.float32)</span><br><span class="line">b2 = tf.Variable(tf.constant(<span class="number">0.01</span>, shape=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.01</span>  <span class="comment"># 学习率</span></span><br><span class="line">epoch = <span class="number">400</span>  <span class="comment"># 循环轮数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练部分</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="keyword">for</span> step, (x_train, y_train) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db):</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># 记录梯度信息</span></span><br><span class="line"></span><br><span class="line">            h1 = tf.matmul(x_train, w1) + b1  <span class="comment"># 记录神经网络乘加运算</span></span><br><span class="line">            h1 = tf.nn.relu(h1)</span><br><span class="line">            y = tf.matmul(h1, w2) + b2</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span></span><br><span class="line">            loss = tf.reduce_mean(tf.square(y_train - y))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算loss对各个参数的梯度</span></span><br><span class="line">        variables = [w1, b1, w2, b2]</span><br><span class="line">        grads = tape.gradient(loss, variables)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 实现梯度更新</span></span><br><span class="line">        <span class="comment"># w1 = w1 - lr * w1_grad tape.gradient是自动求导结果与[w1, b1, w2, b2] 索引为0，1，2，3 </span></span><br><span class="line">        w1.assign_sub(lr * grads[<span class="number">0</span>])</span><br><span class="line">        b1.assign_sub(lr * grads[<span class="number">1</span>])</span><br><span class="line">        w2.assign_sub(lr * grads[<span class="number">2</span>])</span><br><span class="line">        b2.assign_sub(lr * grads[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每20个epoch，打印loss信息</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch:&#x27;</span>, epoch, <span class="string">&#x27;loss:&#x27;</span>, <span class="built_in">float</span>(loss))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测部分</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*******predict*******&quot;</span>)</span><br><span class="line"><span class="comment"># xx在-3到3之间以步长为0.01，yy在-3到3之间以步长0.01,生成间隔数值点</span></span><br><span class="line">xx, yy = np.mgrid[-<span class="number">3</span>:<span class="number">3</span>:<span class="number">.1</span>, -<span class="number">3</span>:<span class="number">3</span>:<span class="number">.1</span>]</span><br><span class="line"><span class="comment"># 将xx , yy拉直，并合并配对为二维张量，生成二维坐标点</span></span><br><span class="line">grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">grid = tf.cast(grid, tf.float32)</span><br><span class="line"><span class="comment"># 将网格坐标点喂入神经网络，进行预测，probs为输出</span></span><br><span class="line">probs = []</span><br><span class="line"><span class="keyword">for</span> x_test <span class="keyword">in</span> grid:</span><br><span class="line">    <span class="comment"># 使用训练好的参数进行预测</span></span><br><span class="line">    h1 = tf.matmul([x_test], w1) + b1</span><br><span class="line">    h1 = tf.nn.relu(h1)</span><br><span class="line">    y = tf.matmul(h1, w2) + b2  <span class="comment"># y为预测结果</span></span><br><span class="line">    probs.append(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取第0列给x1，取第1列给x2</span></span><br><span class="line">x1 = x_data[:, <span class="number">0</span>]</span><br><span class="line">x2 = x_data[:, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># probs的shape调整成xx的样子</span></span><br><span class="line">probs = np.array(probs).reshape(xx.shape)</span><br><span class="line">plt.scatter(x1, x2, color=np.squeeze(Y_c)) <span class="comment">#squeeze去掉纬度是1的纬度,相当于去掉[[&#x27;red&#x27;],[&#x27;&#x27;blue]],内层括号变为[&#x27;red&#x27;,&#x27;blue&#x27;]</span></span><br><span class="line"><span class="comment"># 把坐标xx yy和对应的值probs放入contour&lt;[‘kɑntʊr]&gt;函数，给probs值为0.5的所有点上色  plt点show后 显示的是红蓝点的分界线</span></span><br><span class="line">plt.contour(xx, yy, probs, levels=[<span class="number">.5</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入红蓝点，画出分割线，不包含正则化</span></span><br><span class="line"><span class="comment"># 不清楚的数据，建议print出来查看 </span></span><br></pre></td></tr></table></figure>

<p>最终实现效果如下:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220503111954425.png" alt="image-20220503111954425"></p>
<p>可以看到,分割线轮廓不够平滑,存在过拟合现象.</p>
<p><strong>如果在With结构当中加入L2正则化,将其修改如下:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练部分</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="keyword">for</span> step, (x_train, y_train) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db):</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># 记录梯度信息</span></span><br><span class="line"></span><br><span class="line">            h1 = tf.matmul(x_train, w1) + b1  <span class="comment"># 记录神经网络乘加运算</span></span><br><span class="line">            h1 = tf.nn.relu(h1)</span><br><span class="line">            y = tf.matmul(h1, w2) + b2</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span></span><br><span class="line">            loss_mse = tf.reduce_mean(tf.square(y_train - y))</span><br><span class="line">            <span class="comment"># 添加l2正则化</span></span><br><span class="line">            loss_regularization = []</span><br><span class="line">            <span class="comment"># tf.nn.l2_loss(w)=sum(w ** 2) / 2</span></span><br><span class="line">            loss_regularization.append(tf.nn.l2_loss(w1))</span><br><span class="line">            loss_regularization.append(tf.nn.l2_loss(w2))</span><br><span class="line">            <span class="comment"># 求和</span></span><br><span class="line">            <span class="comment"># 例：x=tf.constant(([1,1,1],[1,1,1]))</span></span><br><span class="line">            <span class="comment">#   tf.reduce_sum(x)</span></span><br><span class="line">            <span class="comment"># &gt;&gt;&gt;6</span></span><br><span class="line">            <span class="comment"># loss_regularization = tf.reduce_sum(tf.stack(loss_regularization))</span></span><br><span class="line">            loss_regularization = tf.reduce_sum(loss_regularization)</span><br><span class="line">            loss = loss_mse + <span class="number">0.03</span> * loss_regularization <span class="comment">#REGULARIZER = 0.03</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算loss对各个参数的梯度</span></span><br><span class="line">        variables = [w1, b1, w2, b2]</span><br><span class="line">        grads = tape.gradient(loss, variables)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 实现梯度更新</span></span><br><span class="line">        <span class="comment"># w1 = w1 - lr * w1_grad</span></span><br><span class="line">        w1.assign_sub(lr * grads[<span class="number">0</span>])</span><br><span class="line">        b1.assign_sub(lr * grads[<span class="number">1</span>])</span><br><span class="line">        w2.assign_sub(lr * grads[<span class="number">2</span>])</span><br><span class="line">        b2.assign_sub(lr * grads[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每200个epoch，打印loss信息</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch:&#x27;</span>, epoch, <span class="string">&#x27;loss:&#x27;</span>, <span class="built_in">float</span>(loss))</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220503113907701.png" alt="image-20220503113907701"></p>
<p>可以看到,加入L2正则化后的曲线更为平缓,有效地缓解了过拟合的情况.</p>
<h2 id="2-6-神经网络参数优化器"><a href="#2-6-神经网络参数优化器" class="headerlink" title="2.6 神经网络参数优化器"></a>2.6 神经网络参数优化器</h2><p>优化器是引导神经网络更新参数的工具.</p>
<h3 id="1-明确一些定义"><a href="#1-明确一些定义" class="headerlink" title="(1)明确一些定义"></a>(1)明确一些定义</h3><p>待优化的参数为w,损失函数为loss,学习率为lr,每次迭代一个batch(每个batch通常包括$2^n$组数据),t表示当前batch迭代的总次数:</p>
<p>1.计算t时刻损失函数关于当前参数的梯度:<br>$$<br>g_t&#x3D;\nabla loss&#x3D;\frac{\partial{loss}}{\partial(w_t)}<br>$$<br>2.计算t时刻一阶动量$m_t$和二阶动量$V_t$</p>
<ul>
<li>一阶动量是与梯度相关的函数</li>
<li>二阶动量是与梯度平方相关的函数</li>
</ul>
<p><strong>不同的优化器其实是定义了不同的一阶动量和二阶动量公式</strong></p>
<p>3.计算t时刻下降梯度:<br>$$<br>\eta_t&#x3D;lr<em>m_t&#x2F;\sqrt{V_t}<br>$$<br>4.计算t+1时刻参数:<br>$$<br>w_{t+1}&#x3D;w_t-\eta_t&#x3D;w_t-lr</em>m_t&#x2F;\sqrt{V_t}<br>$$</p>
<h3 id="2-随机梯度下降SGD"><a href="#2-随机梯度下降SGD" class="headerlink" title="(2)随机梯度下降SGD"></a>(2)随机梯度下降SGD</h3><p><strong>常用的梯度下降法(gt指的是梯度)</strong>,一阶动量定义为梯度,二阶动量恒等于1.<br>$$<br>\begin{cases}<br>m_t&#x3D;g_t \qquad V_t&#x3D;1 \<br>\eta_t&#x3D;lr<em>m_t&#x2F;\sqrt{V_t}&#x3D;lr</em>g_t \<br>w_{t+1}&#x3D;w_t-\eta_t&#x3D;w_t-lr<em>m_t&#x2F;\sqrt{V_t}&#x3D;w_t-lr</em>g_t<br>\end{cases}<br>$$<br>注意到这其实就是之前的梯度下降法公式了.</p>
<p>在Python当中代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w1.assign_sub(lr * grads[<span class="number">0</span>])</span><br><span class="line">b1.assign_sub(lr * grads[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<h4 id="补充-利用Time包来记录程序运行的时间"><a href="#补充-利用Time包来记录程序运行的时间" class="headerlink" title="补充:利用Time包来记录程序运行的时间"></a>补充:利用Time包来记录程序运行的时间</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在之前鸢尾花识别的程序的基础上,加入时间戳机制</span></span><br><span class="line"><span class="keyword">import</span> time  <span class="comment">##1##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练部分开始前</span></span><br><span class="line">now_time = time.time()  <span class="comment">##2##</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    ...</span><br><span class="line"><span class="comment"># 在迭代测试结束后,输出总的花费的时间</span></span><br><span class="line">total_time = time.time() - now_time  <span class="comment">##3##</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;total_time&quot;</span>, total_time)  <span class="comment">##4##</span></span><br></pre></td></tr></table></figure>

<p>(<strong>注意:Time包应该是不需要手动install,直接import应该就有了</strong>)</p>
<p>测试运行结果为:<code>total_time 11.329316139221191</code></p>
<h3 id="3-SGDM-在SGD的基础上增加一阶动量"><a href="#3-SGDM-在SGD的基础上增加一阶动量" class="headerlink" title="(3)SGDM:在SGD的基础上增加一阶动量"></a>(3)SGDM:在SGD的基础上增加一阶动量</h3><p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220503120331880.png" alt="image-20220503120331880"></p>
<p>注意这里β的经验值是0.9(<strong>也就是上一个梯度所占据的比例比较大一些</strong>)</p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sgd-momentun  </span></span><br><span class="line">m_w=<span class="number">0</span>,m_b=<span class="number">0</span></span><br><span class="line">beta=<span class="number">0.9</span></span><br><span class="line">m_w = beta * m_w + (<span class="number">1</span> - beta) * grads[<span class="number">0</span>]</span><br><span class="line">m_b = beta * m_b + (<span class="number">1</span> - beta) * grads[<span class="number">1</span>]</span><br><span class="line">w1.assign_sub(lr * m_w)</span><br><span class="line">b1.assign_sub(lr * m_b)</span><br></pre></td></tr></table></figure>

<p>此时的运行时间为:<code>total_time 13.160895586013794</code></p>
<h3 id="4-Adagrad优化器"><a href="#4-Adagrad优化器" class="headerlink" title="(4)Adagrad优化器"></a>(4)Adagrad优化器</h3><p>引入二阶动量(从开始到现在位置的梯度平方的累计和),<strong>可以对模型中的每个参数分配自适应学习率了</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220503121023177.png" alt="image-20220503121023177"></p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">v_w, v_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># adagrad</span></span><br><span class="line">v_w += tf.square(grads[<span class="number">0</span>])</span><br><span class="line">v_b += tf.square(grads[<span class="number">1</span>])</span><br><span class="line">w1.assign_sub(lr * grads[<span class="number">0</span>] / tf.sqrt(v_w))</span><br><span class="line">b1.assign_sub(lr * grads[<span class="number">1</span>] / tf.sqrt(v_b))</span><br></pre></td></tr></table></figure>

<p>此时的运行时间为:<code>total_time 16.1231586933136</code></p>
<h3 id="5-RMSProp-SGD基础上增加二阶动量"><a href="#5-RMSProp-SGD基础上增加二阶动量" class="headerlink" title="(5)RMSProp,SGD基础上增加二阶动量"></a>(5)RMSProp,SGD基础上增加二阶动量</h3><p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220503121335971.png" alt="image-20220503121335971"></p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">v_w, v_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">beta = <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># rmsprop</span></span><br><span class="line">v_w = beta * v_w + (<span class="number">1</span> - beta) * tf.square(grads[<span class="number">0</span>])</span><br><span class="line">v_b = beta * v_b + (<span class="number">1</span> - beta) * tf.square(grads[<span class="number">1</span>])</span><br><span class="line">w1.assign_sub(lr * grads[<span class="number">0</span>] / tf.sqrt(v_w))</span><br><span class="line">b1.assign_sub(lr * grads[<span class="number">1</span>] / tf.sqrt(v_b))</span><br></pre></td></tr></table></figure>

<p>此时的运行时间为:<code>total_time 16.79032588005066</code></p>
<p>注:此时的损失函数呈现如下的趋势:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220503121724313.png" alt="image-20220503121724313"></p>
<p><strong>根据前面学习的知识,造成这种现象的原因可能是学习率过大,因此我们只需要将学习率调小,即可观察效果:</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220503121844959.png" alt="image-20220503121844959"></p>
<p>可以看到,此时的损失函数就比较正常了.</p>
<h3 id="6-Adam优化器"><a href="#6-Adam优化器" class="headerlink" title="(6)Adam优化器"></a>(6)Adam优化器</h3><p>同时结合SGDM一阶动量和RMSProp的二阶动量,同时引入了修正一阶动量的偏差和二阶动量的偏差:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220503121948416.png" alt="image-20220503121948416"></p>
<p>代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">m_w, m_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">v_w, v_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">beta1, beta2 = <span class="number">0.9</span>, <span class="number">0.999</span></span><br><span class="line">delta_w, delta_b = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">global_step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练部分</span></span><br><span class="line">now_time = time.time()  <span class="comment">##2##</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):  <span class="comment"># 数据集级别的循环，每个epoch循环一次数据集</span></span><br><span class="line">    <span class="keyword">for</span> step, (x_train, y_train) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db):  <span class="comment"># batch级别的循环 ，每个step循环一个batch</span></span><br><span class="line"> <span class="comment">##########################################################################       </span></span><br><span class="line">        global_step += <span class="number">1</span></span><br><span class="line"> <span class="comment">##########################################################################       </span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># with结构记录梯度信息</span></span><br><span class="line">            y = tf.matmul(x_train, w1) + b1  <span class="comment"># 神经网络乘加运算</span></span><br><span class="line">            y = tf.nn.softmax(y)  <span class="comment"># 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）</span></span><br><span class="line">            y_ = tf.one_hot(y_train, depth=<span class="number">3</span>)  <span class="comment"># 将标签值转换为独热码格式，方便计算loss和accuracy</span></span><br><span class="line">            loss = tf.reduce_mean(tf.square(y_ - y))  <span class="comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span></span><br><span class="line">            loss_all += loss.numpy()  <span class="comment"># 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确</span></span><br><span class="line">        <span class="comment"># 计算loss对各个参数的梯度</span></span><br><span class="line">        grads = tape.gradient(loss, [w1, b1])</span><br><span class="line"></span><br><span class="line"><span class="comment">##########################################################################</span></span><br><span class="line"> <span class="comment"># adam</span></span><br><span class="line">        m_w = beta1 * m_w + (<span class="number">1</span> - beta1) * grads[<span class="number">0</span>]</span><br><span class="line">        m_b = beta1 * m_b + (<span class="number">1</span> - beta1) * grads[<span class="number">1</span>]</span><br><span class="line">        v_w = beta2 * v_w + (<span class="number">1</span> - beta2) * tf.square(grads[<span class="number">0</span>])</span><br><span class="line">        v_b = beta2 * v_b + (<span class="number">1</span> - beta2) * tf.square(grads[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        m_w_correction = m_w / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta1, <span class="built_in">int</span>(global_step)))</span><br><span class="line">        m_b_correction = m_b / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta1, <span class="built_in">int</span>(global_step)))</span><br><span class="line">        v_w_correction = v_w / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta2, <span class="built_in">int</span>(global_step)))</span><br><span class="line">        v_b_correction = v_b / (<span class="number">1</span> - tf.<span class="built_in">pow</span>(beta2, <span class="built_in">int</span>(global_step)))</span><br><span class="line"></span><br><span class="line">        w1.assign_sub(lr * m_w_correction / tf.sqrt(v_w_correction))</span><br><span class="line">        b1.assign_sub(lr * m_b_correction / tf.sqrt(v_b_correction))</span><br><span class="line"><span class="comment">##########################################################################</span></span><br></pre></td></tr></table></figure>

<p>此时的运行时间为:<code>total_time 20.095345973968506</code></p>
<p>(<strong>经过测试,这个优化器虽然运行时间会慢一些,但是运行的效果非常不错</strong>)</p>
<h3 id="7-四种优化器的对比"><a href="#7-四种优化器的对比" class="headerlink" title="(7)四种优化器的对比"></a>(7)四种优化器的对比</h3><p>可以参考的博客如下:</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36589234/article/details/89330342?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1.pc_relevant_paycolumn_v3&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1.pc_relevant_paycolumn_v3&utm_relevant_index=2">(11条消息) PyTorch学习之 torch.optim 的6种优化器及优化算法介绍_Line_Walker的博客-CSDN博客_pytorch 优化器</a></p>
<p>后续有时间会继续进行补充</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/02/Tensorflow%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="ChrisZhang">
      <meta itemprop="description" content="我的技美学习之路">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ChrisZhang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/02/Tensorflow%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0-1/" class="post-title-link" itemprop="url">Tensorflow基础学习(1)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-05-02 16:34:57 / 修改时间：22:55:35" itemprop="dateCreated datePublished" datetime="2022-05-02T16:34:57+08:00">2022-05-02</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/tensorflow/" itemprop="url" rel="index"><span itemprop="name">tensorflow</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="TensorFlow-学习-1"><a href="#TensorFlow-学习-1" class="headerlink" title="TensorFlow  学习(1)"></a>TensorFlow  学习(1)</h1><p>参考视频:</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1B7411L7Qt?p=1">【北京大学】Tensorflow2.0_哔哩哔哩_bilibili</a></p>
<h1 id="一-初识神经网络"><a href="#一-初识神经网络" class="headerlink" title="一.初识神经网络"></a>一.初识神经网络</h1><h2 id="1-1-人工智能三学派"><a href="#1-1-人工智能三学派" class="headerlink" title="1.1 人工智能三学派"></a>1.1 人工智能三学派</h2><ul>
<li>行为主义<ul>
<li>基于控制论,构建感知-动作控制系统</li>
</ul>
</li>
<li>符号主义<ul>
<li>基于算数逻辑表达式,可用公式描述(如专家系统)</li>
</ul>
</li>
<li>连接主义<ul>
<li>仿生学,<strong>模仿神经元连接关系</strong></li>
</ul>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501210406652.png" alt="image-20220501210406652"></p>
<h2 id="1-2-神经网络设计过程"><a href="#1-2-神经网络设计过程" class="headerlink" title="1.2 神经网络设计过程"></a>1.2 神经网络设计过程</h2><p>案例:给鸢尾花分类(基于神经网络的方法，具体过程见[鸢尾花分类](# 1.6 鸢尾花分类——数据集读入))</p>
<ul>
<li><strong>神经网络:<strong>采集大量输入特征,以及对应的类别(作为</strong>标签,需要人工标定</strong>)作为数据对构成数据集.</li>
<li>将数据集喂入搭建好的神经网络结构,<strong>网络优化参数得到模型</strong>,模型读入新输入的特征,然后输出识别的结果.</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501210852707.png" alt="image-20220501210852707"></p>
<p><strong>注意,w和b会被随机赋予一些初值,然后根据前向传播和反向传播来调整参数</strong></p>
<ul>
<li>利用<strong>损失函数</strong>来定义预测值$y$与标准答案$y’$的差距<ul>
<li>常用的损失函数有<strong>均方误差</strong>等</li>
</ul>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501211244083.png" alt="image-20220501211244083"></p>
<h3 id="1-尝试一个反向传播使梯度减小的例子"><a href="#1-尝试一个反向传播使梯度减小的例子" class="headerlink" title="(1)尝试一个反向传播使梯度减小的例子"></a>(1)尝试一个反向传播使梯度减小的例子</h3><ul>
<li>注：<strong>如果下例中有难以理解的函数，或许看完后面部分再回来就会更好理解</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w = tf.Variable(tf.constant(<span class="number">5</span>, dtype=tf.float32))  <span class="comment"># 设定参数w的随机初始值为5,可训练</span></span><br><span class="line">lr = <span class="number">0.2</span>  <span class="comment"># 学习率</span></span><br><span class="line">epoch = <span class="number">40</span>  <span class="comment"># 循环迭代次数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):  <span class="comment"># 对数据集循环epoch次</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># with结构到grads框起了梯度的计算过程</span></span><br><span class="line">        loss = tf.square(w + <span class="number">1</span>)  <span class="comment"># 损失函数</span></span><br><span class="line">    grads = tape.gradient(loss, w)  <span class="comment"># .gradient函数告知谁对谁求导</span></span><br><span class="line"></span><br><span class="line">    w.assign_sub(lr * grads)  <span class="comment"># .assign_sub 对变量做自减,即w-=lr*grads</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;After %s epoch,w is %f,loss is %f&quot;</span> % (epoch, w.numpy(), loss))</span><br><span class="line"></span><br><span class="line"><span class="comment"># lr初始值:0.2 请自选学习率0.001,0.009查看收敛过程</span></span><br><span class="line"><span class="comment"># 最终目的:找到loss最小 即w=-1的最优参数w</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="学习率过小或过大的情况"><a href="#学习率过小或过大的情况" class="headerlink" title="学习率过小或过大的情况"></a>学习率过小或过大的情况</h4><p>修改学习率lr的值,测试学习率过小或者过大的结果:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501212540892.png" alt="image-20220501212540892"></p>
<h2 id="1-3-张量生成"><a href="#1-3-张量生成" class="headerlink" title="1.3 张量生成"></a>1.3 张量生成</h2><p>Tensor的意思就是张量,可以直观理解成<strong>多维数组(列表)</strong>,其中张量的维数叫做<strong>阶</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501212810950.png" alt="image-20220501212810950"></p>
<p><code>[</code>的层数可以很好地说明张量的阶数。</p>
<p>其他的数据类型如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.int32,tf.float32,t.constant([True,False]),tf.constant(&quot;Hello,world!&quot;)等</span><br></pre></td></tr></table></figure>



<h3 id="（1）如何创建一个张量？"><a href="#（1）如何创建一个张量？" class="headerlink" title="（1）如何创建一个张量？"></a>（1）如何创建一个张量？</h3><p>使用constant创建一个张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">tf.constant([<span class="number">1</span>,<span class="number">5</span>],dtype=tf.int64)  <span class="comment"># 一阶张量</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.dtype)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br></pre></td></tr></table></figure>

<p>创建张量模板：</p>
<p><code>tf.constant(张量内容,dtype=数据类型（可选）)</code></p>
<ul>
<li>直接打印a会输出张量的所有信息，其中shape的逗号隔开了几个数字，张量就是几维的（比如上例shape隔开了一个数字，就是一维的，2表示张量里有2个元素）</li>
<li>也可以打印<code>a.dtype</code> 和<code>a.shape</code>，如上。</li>
</ul>
<h3 id="（2）Numpy数据转为Tensor"><a href="#（2）Numpy数据转为Tensor" class="headerlink" title="（2）Numpy数据转为Tensor"></a>（2）Numpy数据转为Tensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.arange(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">b = tf.convert_to_tensor(a, dtype=tf.int64)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果：</span></span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span>]</span><br><span class="line">tf.Tensor([<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span>], shape=(<span class="number">5</span>,), dtype=int64)</span><br></pre></td></tr></table></figure>



<h3 id="（3）创建一些固定的张量"><a href="#（3）创建一些固定的张量" class="headerlink" title="（3）创建一些固定的张量"></a>（3）创建一些固定的张量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.zeros([<span class="number">2</span>, <span class="number">3</span>])  <span class="comment"># 参数：维度</span></span><br><span class="line">b = tf.ones([<span class="number">4</span>])</span><br><span class="line">c = tf.fill([<span class="number">2</span>, <span class="number">2</span>], <span class="number">9</span>)  <span class="comment"># 参数:维度,指定值</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">tf.Tensor([[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>][<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]], shape=(<span class="number">2</span>, <span class="number">3</span>),dtype=float32)</span><br><span class="line">tf.Tensor([<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>], shape=(<span class="number">4</span>,),dtype=float32)</span><br><span class="line">tf.Tensor([[<span class="number">9</span> <span class="number">9</span>][<span class="number">9</span> <span class="number">9</span>]], shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=int32)</span><br></pre></td></tr></table></figure>



<h3 id="（4）随机生成随机数"><a href="#（4）随机生成随机数" class="headerlink" title="（4）随机生成随机数"></a>（4）随机生成随机数</h3><ul>
<li>生成正态分布随机数，默认均值0，标准差1</li>
</ul>
<p><code>tf.random.normal(维度,mean=均值,stddev=标准差)</code></p>
<ul>
<li>生成截断式正态分布的随机数(<strong>使得生成的随机数可以更集中一些</strong>)</li>
</ul>
<p><code>tf.random_truncated_normal(维度,mean=均值,stddev=标准差)</code></p>
<p>对截断式正态分布进行补充：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501214431416.png" alt="image-20220501214431416"></p>
<p>示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">d = tf.random.normal([<span class="number">2</span>, <span class="number">2</span>], mean=<span class="number">0.5</span>, stddev=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"></span><br><span class="line">e = tf.random.truncated_normal([<span class="number">2</span>, <span class="number">2</span>], mean=<span class="number">0.5</span>, stddev=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(e)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">tf.Tensor([[<span class="number">0.9194691</span> <span class="number">0.8282855</span>] [<span class="number">2.7526226</span> <span class="number">0.9153044</span>]], shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32)</span><br><span class="line">tf.Tensor([[-<span class="number">0.77154016</span>  <span class="number">0.6888927</span> ] [ <span class="number">1.4981992</span>   <span class="number">1.229878</span>  ]], shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32)</span><br></pre></td></tr></table></figure>



<h3 id="（5）生成均匀分布随机数"><a href="#（5）生成均匀分布随机数" class="headerlink" title="（5）生成均匀分布随机数"></a>（5）生成均匀分布随机数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.random.uniform(维度,minval=最小值,maxval=最大值)</span><br></pre></td></tr></table></figure>

<p>比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f=tf.random.uniform([<span class="number">2</span>,<span class="number">2</span>],minval=<span class="number">0</span>,maxval=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行结果：</span></span><br><span class="line">tf.Tensor([[<span class="number">0.46490908</span> <span class="number">0.5134711</span> ][<span class="number">0.71371794</span> <span class="number">0.76942956</span>]], shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32)</span><br></pre></td></tr></table></figure>



<h2 id="1-4-1-5-Tensorflow常用函数"><a href="#1-4-1-5-Tensorflow常用函数" class="headerlink" title="1.4-1.5 Tensorflow常用函数"></a>1.4-1.5 Tensorflow常用函数</h2><p>注：以下tensor_name表示张量的名字</p>
<p>（1）强制tensor转换为该数据类型</p>
<p><code>tf.cast(tensor_name,dtype=xxx)</code></p>
<p>（2）计算张量维度上元素的最小值</p>
<p><code>tf.reduce_min(tensor_name)</code></p>
<p>（3）计算张量维度上元素的最大值</p>
<p><code>tf.reduce_max(tensor_name)</code></p>
<p>一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x1 = tf.constant([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], dtype=tf.float64)</span><br><span class="line"><span class="built_in">print</span>(x1)</span><br><span class="line"></span><br><span class="line">x2 = tf.cast(x1, tf.int32)</span><br><span class="line"><span class="built_in">print</span>(x2)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tf.reduce_min(x2), tf.reduce_max(x2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">tf.Tensor([<span class="number">1.</span> <span class="number">2.</span> <span class="number">3.</span>], shape=(<span class="number">3</span>,), dtype=float64)</span><br><span class="line">tf.Tensor([<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>], shape=(<span class="number">3</span>,), dtype=int32)</span><br><span class="line">tf.Tensor(<span class="number">1</span>, shape=(), dtype=int32) tf.Tensor(<span class="number">3</span>, shape=(), dtype=int32)</span><br></pre></td></tr></table></figure>



<p>（4）<strong>axis</strong>函数</p>
<p>在一个二维张量或数组中，可以通过调整axis&#x3D;0或1来控制执行维度，（<strong>axis&#x3D;0表示跨行（经度，down），axis&#x3D;1表示跨列（纬度，across）</strong>）</p>
<p>如果不指定axis，则所有元素参与运算。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502104026064.png" alt="image-20220502104026064"></p>
<p>（5）计算张量沿着指定维度的平均值</p>
<p><code>tf.reduce_mean(tensor_name,axis=XXX)</code></p>
<p>（6）计算张量沿着指定维度的和</p>
<p><code>tf.reduce_sum(tensor_name,axis=XXX)</code></p>
<p>一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>]])  <span class="comment"># 两维的</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tf.reduce_mean(x))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tf.reduce_sum(x, axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行结果</span></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">2</span> <span class="number">3</span>]], shape=(<span class="number">2</span>, <span class="number">3</span>), dtype=int32)</span><br><span class="line"></span><br><span class="line">tf.Tensor(<span class="number">2</span>, shape=(), dtype=int32)</span><br><span class="line">tf.Tensor([<span class="number">6</span> <span class="number">7</span>], shape=(<span class="number">2</span>,), dtype=int32)  <span class="comment"># 沿着横向移动，所以每行求个和</span></span><br></pre></td></tr></table></figure>



<h3 id="（7）标记可训练"><a href="#（7）标记可训练" class="headerlink" title="（7）标记可训练"></a><strong>（7）标记可训练</strong></h3><p>被标记的变量会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记待训练参数。</p>
<p><code>tf.Variable(初始值)</code></p>
<p>例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w=tf.Variable(tf.random.normal[<span class="number">2</span>,<span class="number">2</span>],mean=<span class="number">0</span>,stddev=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>



<p>（8）常用数学运算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.add(tensor1,tensor2)</span><br><span class="line">tf.subtract(tensor1,tensor2)</span><br><span class="line">tf.multiply(tensor1,tensor2)</span><br><span class="line">tf.divide(tensor1,tensor2)</span><br><span class="line"></span><br><span class="line">tf.square(tensor1)</span><br><span class="line">tf.<span class="built_in">pow</span>(tensor1,n)</span><br><span class="line">tf,sqrt(tensor1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵乘法</span></span><br><span class="line">tf.matmul(matrix1,matrix2)</span><br></pre></td></tr></table></figure>

<p><strong>只有维度相同的张量才能进行四则运算。</strong></p>
<h3 id="（9）特征配对函数"><a href="#（9）特征配对函数" class="headerlink" title="（9）特征配对函数"></a>（9）特征配对函数</h3><p><code>tf.data.Dateset.from_tensor_slices((输入特征，标签))</code></p>
<p><strong>切分传入张量的第一维度，生成输入特征标签对，构建数据集。</strong></p>
<p>注：Numpy和Tensor格式都可以用该语句读入数据</p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">features = tf.constant([<span class="number">12</span>, <span class="number">23</span>, <span class="number">10</span>, <span class="number">17</span>])</span><br><span class="line">labels = tf.constant([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices((features, labels))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dataset)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> element <span class="keyword">in</span> dataset:</span><br><span class="line">    <span class="built_in">print</span>(element)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 输出结果如下：</span></span><br><span class="line">(&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">9</span>, shape=(), dtype=int32, numpy=<span class="number">12</span>&gt;, &lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">10</span>, shape=(), dtype=int32, numpy=<span class="number">0</span>&gt;)</span><br><span class="line">(&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">11</span>, shape=(), dtype=int32, numpy=<span class="number">23</span>&gt;, &lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">12</span>, shape=(), dtype=int32, numpy=<span class="number">1</span>&gt;)</span><br><span class="line">(&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">13</span>, shape=(), dtype=int32, numpy=<span class="number">10</span>&gt;, &lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">14</span>, shape=(), dtype=int32, numpy=<span class="number">1</span>&gt;)</span><br><span class="line">(&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">15</span>, shape=(), dtype=int32, numpy=<span class="number">17</span>&gt;, &lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">16</span>, shape=(), dtype=int32, numpy=<span class="number">0</span>&gt;)</span><br></pre></td></tr></table></figure>

<p>可以看到，数据与标签进行了很好地匹配，这就是我们后续计算的基础。</p>
<h3 id="（10）实现函数对某参数的求导运算"><a href="#（10）实现函数对某参数的求导运算" class="headerlink" title="（10）实现函数对某参数的求导运算"></a>（10）实现函数对某参数的求导运算</h3><p><code>tf.GradientTape</code></p>
<ul>
<li>with结构记录计算过程，gradient求出张量的梯度</li>
</ul>
<p>结构如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    若干个计算过程</span><br><span class="line">grad=tape.gradient(函数,对谁求导)</span><br></pre></td></tr></table></figure>

<p>比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    w = tf.Variable(tf.constant(<span class="number">3.0</span>))</span><br><span class="line">    loss = tf.<span class="built_in">pow</span>(w, <span class="number">2</span>)</span><br><span class="line">grad = tape.gradient(loss, w)</span><br><span class="line"><span class="built_in">print</span>(grad)</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502110601376.png" alt="image-20220502110601376"></p>
<h3 id="（11）enumerate枚举"><a href="#（11）enumerate枚举" class="headerlink" title="（11）enumerate枚举"></a>（11）enumerate枚举</h3><p>这个函数是python的内建函数，可以遍历每个元素（如列表，元组或字符串），并组合为<strong>索引  元素</strong>，常在for循环中使用，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">seq = [<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;three&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i, element <span class="keyword">in</span> <span class="built_in">enumerate</span>(seq):</span><br><span class="line">    <span class="built_in">print</span>(i, element)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="number">0</span> one</span><br><span class="line"><span class="number">1</span> two</span><br><span class="line"><span class="number">2</span> three</span><br></pre></td></tr></table></figure>



<h3 id="（12）分类问题常用——独热编码"><a href="#（12）分类问题常用——独热编码" class="headerlink" title="（12）分类问题常用——独热编码"></a>（12）分类问题常用——独热编码</h3><p><code>tf.one_hot</code></p>
<p><strong>独热编码（one-hot encoding）：在分类问题中，常用独热码做标签，标记类别：1表示是，0表示非。</strong></p>
<p>比如：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502111159869.png" alt="image-20220502111159869"></p>
<p>（标签是1（杂色鸢尾），百分之0是狗尾草鸢尾，百分之0是弗吉尼亚鸢尾，百分之百是杂色鸢尾）</p>
<ul>
<li><code>tf.one_hot()</code>函数将待转换数据，转换为one_hot形式的数据输出.</li>
</ul>
<p><code>tf.one_hot(待转换数据,depth=几分类)</code></p>
<p>比如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">classes = <span class="number">3</span>  <span class="comment"># 三分类问题</span></span><br><span class="line">labels = tf.constant([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])  <span class="comment"># 输入元素最小值为0,最大值为2</span></span><br><span class="line">output = tf.one_hot(labels, depth=classes)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果:</span></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]], shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=float32)</span><br></pre></td></tr></table></figure>



<h3 id="（13）tf-nn-softmax"><a href="#（13）tf-nn-softmax" class="headerlink" title="（13）tf.nn.softmax"></a>（13）tf.nn.softmax</h3><p>对于分类问题，神经网络完成前向传播，计算出了每种类型的可能性大小，<strong>但这些可能性输出需要符合概率分布，如下述示意图：</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502111901887.png" alt="image-20220502111901887"></p>
<p>以图中为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">y = tf.constant([<span class="number">1.01</span>, <span class="number">2.01</span>, -<span class="number">0.66</span>])</span><br><span class="line">y_pro = tf.nn.softmax(y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;After softmax,y_pro is:&quot;</span>, y_pro)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果：</span></span><br><span class="line">After softmax,y_pro <span class="keyword">is</span>: tf.Tensor([<span class="number">0.25598174</span> <span class="number">0.69583046</span> <span class="number">0.0481878</span> ], shape=(<span class="number">3</span>,), dtype=float32)</span><br></pre></td></tr></table></figure>



<h3 id="（14）自减函数"><a href="#（14）自减函数" class="headerlink" title="（14）自减函数"></a>（14）自减函数</h3><ul>
<li>该函数要求数据是可训练的（需要用tf.Variable先定义可训练）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w=tf.Variable(<span class="number">4</span>)</span><br><span class="line">w.assign_sub(<span class="number">1</span>)  <span class="comment"># 即w-=1</span></span><br><span class="line"><span class="built_in">print</span>(w)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=() dtype=int32, numpy=<span class="number">3</span>&gt;</span><br></pre></td></tr></table></figure>



<h3 id="（15）返回张量沿指定维度最大值的索引"><a href="#（15）返回张量沿指定维度最大值的索引" class="headerlink" title="（15）返回张量沿指定维度最大值的索引"></a>（15）返回张量沿指定维度最大值的索引</h3><p><code>tf.argmax(tensor_name,axis=XXX)</code></p>
<p>比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">test = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">8</span>, <span class="number">7</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="built_in">print</span>(test)</span><br><span class="line"><span class="built_in">print</span>(tf.argmax(test, axis=<span class="number">0</span>))  <span class="comment"># 返回每一列(经度)的最大值的索引</span></span><br><span class="line"><span class="built_in">print</span>(tf.argmax(test, axis=<span class="number">1</span>))  <span class="comment"># 返回每一行(纬度)的最大值的索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果：</span></span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">4</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">8</span> <span class="number">7</span> <span class="number">2</span>]]</span><br><span class="line">tf.Tensor([<span class="number">3</span> <span class="number">3</span> <span class="number">1</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">2</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span>], shape=(<span class="number">4</span>,), dtype=int64)</span><br></pre></td></tr></table></figure>



<h2 id="1-6-鸢尾花分类——数据集读入"><a href="#1-6-鸢尾花分类——数据集读入" class="headerlink" title="1.6 鸢尾花分类——数据集读入"></a>1.6 鸢尾花分类——数据集读入</h2><p>数据集介绍：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502113123185.png" alt="image-20220502113123185"></p>
<p><strong>这个用例需要用到Pandas包和sklearn包，可以通过pip install来进行安装（有可能Pycharm无法找到sklearn，所以用pip install会比较稳妥），具体可以看[Tensorflow安装]这一篇博客</strong></p>
<p>可以用sklearn包datasets读入数据集，语法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">x_data = datasets.load_iris().data</span><br><span class="line">y_data = datasets.load_iris().target</span><br></pre></td></tr></table></figure>

<p>完整的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">x_data = datasets.load_iris().data</span><br><span class="line">y_data = datasets.load_iris().target</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_data from datasets:\n&quot;</span>, x_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_data from datasets:\n&quot;</span>, y_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了美观,采用表格的方式输出</span></span><br><span class="line">x_data = DataFrame(x_data, columns=[<span class="string">&#x27;花萼长度&#x27;</span>, <span class="string">&#x27;花萼宽度&#x27;</span>, <span class="string">&#x27;花瓣长度&#x27;</span>, <span class="string">&#x27;花瓣宽度&#x27;</span>])</span><br><span class="line">pd.set_option(<span class="string">&#x27;display.unicode.east_asian_width&#x27;</span>, <span class="literal">True</span>)  <span class="comment"># 设置列名对齐</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_data add index: \n&quot;</span>, x_data)</span><br><span class="line"></span><br><span class="line">x_data[<span class="string">&#x27;类别&#x27;</span>] = y_data  <span class="comment"># 新加一列,列标签为&quot;类别&quot;,数据为y_data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_data add a column:\n&quot;</span>, x_data)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>最后可以看到的数据集+标签效果如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502115457987.png" alt="image-20220502115457987"></p>
<h2 id="1-7-神经网络实现鸢尾花分类-重要！"><a href="#1-7-神经网络实现鸢尾花分类-重要！" class="headerlink" title="1.7 神经网络实现鸢尾花分类(重要！)"></a>1.7 神经网络实现鸢尾花分类(重要！)</h2><p>具体的步骤如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502115605657.png" alt="image-20220502115605657"></p>
<p>整体过程如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.数据集读入(一共150组)</span></span><br><span class="line">x_data = datasets.load_iris().data</span><br><span class="line">y_data = datasets.load_iris().target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.数据集乱序(模拟人脑乱序处理信息)</span></span><br><span class="line">np.random.seed(<span class="number">116</span>)  <span class="comment"># 使用相同的seed,使输入特征/标签一一对应</span></span><br><span class="line">np.random.shuffle(x_data)</span><br><span class="line">np.random.seed(<span class="number">116</span>)</span><br><span class="line">np.random.shuffle(y_data)</span><br><span class="line">tf.random.set_seed(<span class="number">116</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.数据集分出永不相见的训练集和测试集(训练集和测试集不能重复)</span></span><br><span class="line">x_train = x_data[:-<span class="number">30</span>]  <span class="comment"># 前120个数据取出来作为训练集</span></span><br><span class="line">y_train = y_data[:-<span class="number">30</span>]</span><br><span class="line">x_test = x_data[-<span class="number">30</span>:]  <span class="comment"># 后30个数据取出来作为测试集</span></span><br><span class="line">y_test = y_data[-<span class="number">30</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换x的数据类型,否则后面矩阵相乘时会因数据类型不一致而报错</span></span><br><span class="line">x_train = tf.cast(x_train, tf.float32)</span><br><span class="line">x_test = tf.cast(x_test, tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.配成[输入特征,标签]对,每次喂入一个batch</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="number">32</span>)</span><br><span class="line">test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class="number">32</span>)  <span class="comment"># 每32组打包成一个batch</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.定义神经网络的所有可训练参数,只有一层网络,输入结点四个,输出结点3个(3分类)</span></span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([<span class="number">4</span>, <span class="number">3</span>], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br><span class="line">b1 = tf.Variable(tf.random.truncated_normal([<span class="number">3</span>], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一些超参数</span></span><br><span class="line">lr = <span class="number">0.1</span>  <span class="comment"># 学习率</span></span><br><span class="line"><span class="comment"># 下面两个是画图用的</span></span><br><span class="line">train_loss_results = []  <span class="comment"># 每轮的loss记录进去</span></span><br><span class="line">test_acc = []  <span class="comment"># 每轮的准确率记录进去</span></span><br><span class="line">epoch = <span class="number">500</span>  <span class="comment"># 迭代次数</span></span><br><span class="line">loss_all = <span class="number">0</span>  <span class="comment"># 每轮四个step(150数据量,32个一个batch,共四组batch训练一次,记录四个batch生成的4个loss的和)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.嵌套循环迭代,with结构更新参数,显示当前loss</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):  <span class="comment"># 数据集级别迭代</span></span><br><span class="line">    <span class="keyword">for</span> step, (x_train, y_train) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db):  <span class="comment"># 针对batch级别的迭代</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># 记录梯度信息</span></span><br><span class="line">            y = tf.matmul(x_train, w1) + b1  <span class="comment"># 神经网络乘加运算:32*4 4*3,可以相乘</span></span><br><span class="line">            y = tf.nn.softmax(y)  <span class="comment"># 使得输出y符合概率分布(这样才能和独热码求均方误差loss)</span></span><br><span class="line">            y_ = tf.one_hot(y_train, depth=<span class="number">3</span>)  <span class="comment"># 转换为独热码模式方便求loss</span></span><br><span class="line">            loss = tf.reduce_mean(tf.square(y_ - y))  <span class="comment"># reduce_mean计算沿某一维度的平均值,默认所有</span></span><br><span class="line">            loss_all += loss.numpy()</span><br><span class="line">        <span class="comment"># 计算loss对各个参数的梯度</span></span><br><span class="line">        grads = tape.gradient(loss, [w1, b1])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 实现梯度下降</span></span><br><span class="line">        w1.assign_sub(lr * grads[<span class="number">0</span>])  <span class="comment"># 参数w1自更新,下面类似,每个batch训练完更新一次</span></span><br><span class="line">        b1.assign_sub(lr * grads[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个epoch,打印loss信息</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch&#123;&#125;,loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch, loss_all / <span class="number">4</span>))  <span class="comment"># 因为一次迭代是4个batch,所以这里/4</span></span><br><span class="line">    train_loss_results.append(loss_all / <span class="number">4</span>)</span><br><span class="line">    loss_all = <span class="number">0</span>  <span class="comment"># 归零,方便下次epoch的计算</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试部分,每次迭代都测试一下(主要是为了看学习过程)</span></span><br><span class="line">    <span class="comment"># total_correct为预测对的样本个数,total_number为测试的总样本数,都初始化为0</span></span><br><span class="line">    total_correct, total_number = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_test, y_test <span class="keyword">in</span> test_db:</span><br><span class="line">        <span class="comment"># 使用更新后的参数预测</span></span><br><span class="line">        y = tf.matmul(x_test, w1) + b1</span><br><span class="line">        y = tf.nn.softmax(y)</span><br><span class="line">        pred = tf.argmax(y, axis=<span class="number">1</span>)  <span class="comment"># 返回y中最大值的索引,即预测的分类</span></span><br><span class="line">        pred = tf.cast(pred, dtype=y_test.dtype)  <span class="comment"># 转换数据类型</span></span><br><span class="line">        <span class="comment"># 若分类正确,则correct=1,否则为0,将bool的结果转换为int型</span></span><br><span class="line">        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)</span><br><span class="line">        <span class="comment"># 将每个batch的correct数加起来</span></span><br><span class="line">        correct = tf.reduce_sum(correct)  <span class="comment"># reduce_sum计算张量沿着指定维度的和,默认全维度</span></span><br><span class="line">        <span class="comment"># 将所有batch中的correct数加起来</span></span><br><span class="line">        total_correct += <span class="built_in">int</span>(correct)</span><br><span class="line">        <span class="comment"># total_number为测试的总样本数,也就是x_test的行数</span></span><br><span class="line">        total_number += x_test.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 总的准确率计算</span></span><br><span class="line">    acc = total_correct / total_number</span><br><span class="line">    test_acc.append(acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test_acc:&quot;</span>, acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;--------------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制loss曲线</span></span><br><span class="line">plt.title(<span class="string">&#x27;Loss Function Curve&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.plot(train_loss_results, label=<span class="string">&quot;$Loss$&quot;</span>)  <span class="comment"># 逐点画出train_loss_results并连线.连线图标是Loss</span></span><br><span class="line">plt.legend()  <span class="comment"># 画出曲线坐标</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制Accuracy曲线</span></span><br><span class="line">plt.title(<span class="string">&#x27;Acc Curve&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Acc&#x27;</span>)</span><br><span class="line">plt.plot(test_acc, label=<span class="string">&quot;$Accuracy&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><strong>针对上述的代码，有几点注意事项：</strong></p>
<p>1、我们所搭建的神经网络如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502162605534.png"></p>
<p>（四层输入层，三层输出层，以全连接的形式）</p>
<p>2、注意所有的输入和输出（比如xtest，xtrain，ytest，ytrain）都是矩阵的形式，每个矩阵当中包含一整个batch的数据集，因此在做矩阵相乘的时候要注意维度是否匹配；</p>
<p>3、<strong>如果对于矩阵乘法仍感觉迷惑的话，需要复习一下与线性代数有关的内容，可以手动计算一下各个矩阵的维度和计算结果的维度</strong></p>
<p>最后的输出结果如下：</p>
<p>（1）首先是运行结果：可以看到，控制台输出的准确率越来越高，最后可以达到百分之百的准确率，同时损失函数值越来越小：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502162245222.png" alt="image-20220502162245222"></p>
<p>（2）观察损失函数值随迭代次数的变化，可以看到上述趋势：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502162326718.png"></p>
<p>（3）观察准确率随迭代次数的变化，可以看到正确的趋势：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502162402489.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/01/Tensorflow%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="ChrisZhang">
      <meta itemprop="description" content="我的技美学习之路">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ChrisZhang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/01/Tensorflow%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/" class="post-title-link" itemprop="url">Tensorflow安装配置步骤</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-05-01 20:41:25" itemprop="dateCreated datePublished" datetime="2022-05-01T20:41:25+08:00">2022-05-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-05-03 10:48:13" itemprop="dateModified" datetime="2022-05-03T10:48:13+08:00">2022-05-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Tensorflow安装配置"><a href="#Tensorflow安装配置" class="headerlink" title="Tensorflow安装配置"></a>Tensorflow安装配置</h2><h3 id="一-Anaconda和Conda配置"><a href="#一-Anaconda和Conda配置" class="headerlink" title="一.Anaconda和Conda配置"></a>一.Anaconda和Conda配置</h3><h4 id="1-安装Anaconda"><a href="#1-安装Anaconda" class="headerlink" title="1.安装Anaconda"></a>1.安装Anaconda</h4><p><a target="_blank" rel="noopener" href="https://www.anaconda.com/products/distribution#Downloads">Anaconda | Anaconda Distribution</a></p>
<p>打开之后,按照自己机器对应的版本进行下载:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220501104842870.png"></p>
<p>在合适的路径安装即可.</p>
<h4 id="2-配置Anaconda环境变量"><a href="#2-配置Anaconda环境变量" class="headerlink" title="2.配置Anaconda环境变量"></a>2.配置Anaconda环境变量</h4><p>参考博客:<a target="_blank" rel="noopener" href="https://blog.csdn.net/qhd1994/article/details/111299942">https://blog.csdn.net/qhd1994/article/details/111299942</a></p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220501144954539.png" alt="image-20220501144954539"></p>
<p>(<strong>将Anaconda的Scripts文件夹路径配置在环境变量下</strong>)</p>
<p>此时打开CMD，输入conda并回车，查看是否能看到如下结果，如果能看到说明配置成功：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220501145045582.png" alt="image-20220501145045582"></p>
<p>接下来，安装pycharm（过程略，网上可以找到不少符合要求的教程）</p>
<h3 id="二-安装Pycharm-略"><a href="#二-安装Pycharm-略" class="headerlink" title="二.安装Pycharm(略)"></a>二.安装Pycharm(略)</h3><h3 id="三-创建虚拟环境"><a href="#三-创建虚拟环境" class="headerlink" title="三.创建虚拟环境"></a>三.创建虚拟环境</h3><p>（1）打开cmd，输入<code>conda create --name tf_2.3 python=3.9 </code>创建虚拟环境（根据自己的python版本来决定，可以在pycharm当中查看）</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220501145425949.png" alt="image-20220501145425949"></p>
<p>(<strong>这里要考虑Anaconda和conda的版本,如果不是最新版的话建议不要安装过新的Python版本,下述的Tensorflow同</strong>)</p>
<p>等待安装完成即可。</p>
<p>（2）检验虚拟环境是否创建成功，使用<code>conda env list</code>命令查看虚拟环境列表：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220501145545502.png" alt="image-20220501145545502"></p>
<p>可以看到，tf_2.3正是我们刚才配置的虚拟环境。</p>
<p>（3）在上一步基础上利用<code>activate tf_2.3 </code>命令进入到虚拟环境；</p>
<p>（4）安装tensorflow-gpu（如果不带gpu，则默认是cpu版本），代码<code>conda install tensorflow-gpu=2.5</code>（<strong>注意这里的版本要和python的版本保持一致，同时最好兼顾一下anaconda的版本问题</strong>）</p>
<p>查看conda，tensorflow和python版本是否匹配：<a target="_blank" rel="noopener" href="https://blog.csdn.net/K1052176873/article/details/114526086">(25条消息) 2021最新：TensorFlow各个GPU版本CUDA和cuDNN对应版本整理(最简洁)_Kakaluotuo的博客-CSDN博客_tensorflow对应的cuda版本</a></p>
<p>接下来，只需要通过上述指令下载安装即可。等待安装之后进行测试。</p>
<h4 id="测试环节"><a href="#测试环节" class="headerlink" title="测试环节"></a>测试环节</h4><ul>
<li>先输入python，查看python的版本是否有问题：</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220501151335646.png" alt="image-20220501151335646"></p>
<p>可以看到，此时Python的版本是3.9，与之前指令中所写的保持一致</p>
<p>接下来输入<code>from tensorflow.python.client import device_lib</code>回车；接着输入<code>print(device_lib.list_local_devices())</code>,并回车</p>
<p><strong>如果看到一个CPU一个GPU表示tensorflow安装成功！</strong>安装成功的界面如下图所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220501151455330.png" alt="image-20220501151455330"></p>
<h3 id="四-设置Pycharm工程为虚拟环境"><a href="#四-设置Pycharm工程为虚拟环境" class="headerlink" title="四.设置Pycharm工程为虚拟环境"></a>四.设置Pycharm工程为虚拟环境</h3><p>（1）进入到Pycharm的Settings界面，查看当前的虚拟环境：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220501202411182.png" alt="image-20220501202411182"></p>
<p>如果没有找到之前配置的虚拟环境,则新建一个环境并进行编辑:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220501202534903.png" alt="image-20220501202534903"></p>
<p><strong>注意:选择Existing enviroment,然后Interpreter选择虚拟环境当中的python.exe,Conda executable选择Conda安装路径的conda.exe文件.</strong></p>
<p><strong>注：可能需要进行升级Conda的操作</strong>（针对无法在Pycharm当中看到Conda虚拟环境的3.9版本的情况，升级后可能也看不到但是如果不升级虚拟环境可能无法被识别，会报错）：</p>
<p>参考博客：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_51713776/article/details/109501111">(25条消息) 在anaconda中创建Python3.9的环境_YKL66的博客-CSDN博客_anaconda支持python3.9吗</a></p>
<p><strong>一定要进行升级，否则会报错！！（当然也可以不使用这么新的版本）</strong></p>
<p>核心指令：</p>
<p><code>conda update conda</code></p>
<p>（2）将工程部署到该环境下，然后新建脚本<code>test1.py</code>，查看tensorflow是否安装成功：</p>
<p>以下是实例程序：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># View more python learning tutorial on my Youtube and Youku channel!!!</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg</span></span><br><span class="line"><span class="comment"># Youku video tutorial: http://i.youku.com/pythontutorial</span></span><br><span class="line"> </span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_layer</span>(<span class="params">inputs, in_size, out_size, activation_function=<span class="literal">None</span></span>):</span><br><span class="line">    Weights = tf.Variable(tf.random_normal([in_size, out_size]))</span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>)</span><br><span class="line">    Wx_plus_b = tf.matmul(inputs, Weights) + biases</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b)</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Make up some real data</span></span><br><span class="line">x_data = np.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">300</span>)[:, np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data) - <span class="number">0.5</span> + noise</span><br><span class="line"> </span><br><span class="line"><span class="comment">##plt.scatter(x_data, y_data)</span></span><br><span class="line"><span class="comment">##plt.show()</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># define placeholder for inputs to network</span></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># add hidden layer</span></span><br><span class="line">l1 = add_layer(xs, <span class="number">1</span>, <span class="number">10</span>, activation_function=tf.nn.relu)</span><br><span class="line"><span class="comment"># add output layer</span></span><br><span class="line">prediction = add_layer(l1, <span class="number">10</span>, <span class="number">1</span>, activation_function=<span class="literal">None</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># the error between prediction and real data</span></span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction), reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line"><span class="comment"># important step</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># tf.initialize_all_variables() no long valid from</span></span><br><span class="line"><span class="comment"># 2017-03-02 if using tensorflow &gt;= 0.12</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">int</span>((tf.__version__).split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">1</span>]) &lt; <span class="number">12</span> <span class="keyword">and</span> <span class="built_in">int</span>((tf.__version__).split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]) &lt; <span class="number">1</span>:</span><br><span class="line">    init = tf.initialize_all_variables()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># plot the real data</span></span><br><span class="line">fig = plt.figure()<span class="comment">#画出幕布，以便在上面画图</span></span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment">#将幕布分为1行1列，然后从左往右从上到下在第1个子网格中画图</span></span><br><span class="line">ax.scatter(x_data, y_data)<span class="comment">#画出真实数据的点状图</span></span><br><span class="line">plt.ion()</span><br><span class="line"><span class="comment">#如果不加这个，每画完一条线程序会暂停，加了后会一直画，如果一直画不是会很</span></span><br><span class="line"><span class="comment">#密密麻麻看不清吗，后面 有语句会移除当前线条防止出现该情况</span></span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#显示画好后的散点图，plt这个函数如果画完第一次整个程序就暂停了，</span></span><br><span class="line"><span class="comment">#如果想连续画图就要加上plt.ion（）</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># training</span></span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># to visualize the result and improvement</span></span><br><span class="line">        <span class="keyword">try</span>:<span class="comment">#尝试以下语句</span></span><br><span class="line">            ax.lines.remove(lines[<span class="number">0</span>])<span class="comment">#抹除画出来的第一条线</span></span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            <span class="comment">#如果没有的话，忽略第一次的错误，因为这时还没画第一条线</span></span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        prediction_value = sess.run(prediction, feed_dict=&#123;xs: x_data&#125;)<span class="comment">#预测值</span></span><br><span class="line">        <span class="comment"># plot the prediction</span></span><br><span class="line">        lines = ax.plot(x_data, prediction_value, <span class="string">&#x27;r-&#x27;</span>, lw=<span class="number">5</span>)</span><br><span class="line">        <span class="comment">#画出预测线，r-表红色，lw表粗度为5</span></span><br><span class="line">        plt.pause(<span class="number">1</span>)<span class="comment">#每画完一条线暂停一秒</span></span><br></pre></td></tr></table></figure>

<p>需要下载Matplotlib包，如果运行没有问题的话tensorflow的环境应该就配好了。</p>
<h3 id="五-配置环境的好处"><a href="#五-配置环境的好处" class="headerlink" title="五.配置环境的好处"></a>五.配置环境的好处</h3><p>(1)针对不同的开发需求,我们可以使用不同的虚拟环境,这样以后再遇到需要Tensorflow开发的时候,就可以直接使用配置好的虚拟环境即可.</p>
<p>(2)虚拟环境当中可以随时增加和删除包,有如下两种方式:</p>
<ul>
<li>(a)直接使用Pycharm安装,方法如下:</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220501203046252.png" alt="image-20220501203046252"></p>
<ul>
<li>(b)使用pip 命令安装(<strong>实测有的时候这种方法更快捷一些</strong>):</li>
</ul>
<p><strong>方法1：用CMD进行安装</strong></p>
<p>首先,进入到虚拟环境当中:</p>
<p>打开CMD,输入<code>conda env list</code>,显示虚拟环境列表。比如现在我们想要给tf_2.3环境配置某个包，可以使用<code>activate tf_2.3</code>进入这个环境当中；</p>
<p>接下来，使用<code>pip install xxx</code>就可以安装某个包了。</p>
<p>这里示例如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220501203700948.png" alt="image-20220501203700948"></p>
<p>安装成功之后，可以在Pycharm当中查看：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220501203814115.png" alt="image-20220501203814115"></p>
<p>可以看到，安装也是成功的。</p>
<p><strong>方法2：直接用Pycharm的Terminal进行安装</strong>：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502114922361.png" alt="image-20220502114922361"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/01/%E8%87%AA%E5%B7%B1%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="ChrisZhang">
      <meta itemprop="description" content="我的技美学习之路">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ChrisZhang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/01/%E8%87%AA%E5%B7%B1%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/" class="post-title-link" itemprop="url">自己搭建博客教程</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-05-01 12:15:25" itemprop="dateCreated datePublished" datetime="2022-05-01T12:15:25+08:00">2022-05-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-05-03 00:29:32" itemprop="dateModified" datetime="2022-05-03T00:29:32+08:00">2022-05-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="自己搭建博客"><a href="#自己搭建博客" class="headerlink" title="自己搭建博客"></a>自己搭建博客</h2><p>参考文章:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/102592286">从零开始搭建个人博客（超详细） - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26625249">GitHub+Hexo 搭建个人网站详细教程 - 知乎 (zhihu.com)</a></p>
<h3 id="一-注册Github"><a href="#一-注册Github" class="headerlink" title="一.注册Github"></a>一.注册Github</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/103268406">个人博客第1篇——注册GitHub - 知乎 (zhihu.com)</a></p>
<h3 id="二-下载Git"><a href="#二-下载Git" class="headerlink" title="二.下载Git"></a>二.下载Git</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/103325381">个人博客第2篇——Git 2.25.0详细安装步骤 - 知乎 (zhihu.com)</a></p>
<h3 id="三-绑定Github并提交文件"><a href="#三-绑定Github并提交文件" class="headerlink" title="三.绑定Github并提交文件"></a>三.绑定Github并提交文件</h3><ul>
<li>这里利用SSH来完成Github的绑定并提交文件.</li>
</ul>
<blockquote>
<p>SSH（安全外壳协议，Secure Shell 的缩写）是建立在应用层基础上的安全协议。<strong>SSH 是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议，利用 SSH 协议可以有效防止远程管理过程中的信息泄露问题。</strong>简单来说，SSH就是保障你的账户安全，将你的数据加密压缩，不仅防止其他人截获你的数据，还能加快传输速度。如果想详细了解的话，可以看这篇文章：<a href="https://link.zhihu.com/?target=https://blog.csdn.net/qq_35246620/article/details/54317740">详述 SSH 的原理及其应用 - CSDN</a></p>
</blockquote>
<h4 id="1-绑定Github"><a href="#1-绑定Github" class="headerlink" title="1.绑定Github"></a>1.绑定Github</h4><p>如果想要使用git上传文件,需要先利用SSH登录远程主机,<strong>登陆方式分为口令登录和公钥登录</strong>:</p>
<ul>
<li>口令登录每次都要输入密码,比较麻烦</li>
<li>公钥登录需要在Github上添加<strong>SSH key配置</strong>,这里采用第二种方式</li>
</ul>
<h5 id="1-检查本机是否安装了SSH"><a href="#1-检查本机是否安装了SSH" class="headerlink" title="(1)检查本机是否安装了SSH:"></a>(1)<strong>检查本机是否安装了SSH:</strong></h5><p>打开Git Bash,输入ssh查看结果:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220430153207313.png"></p>
<p>上图表示已经安装了SSH,接下来输入<code>ssh-keygen -t rsa</code>(注意空格),表示指定RSA算法生成密钥.按下回车后按照提示进行操作(<strong>比如说是否要覆盖掉之前的文件等</strong>).</p>
<blockquote>
<p>注:在Git当中,复制粘贴的快捷键是Ctrl+Insert和Shift+Insert,也可以通过鼠标右键复制.</p>
</blockquote>
<ul>
<li><strong>找到密钥id_rsa和公钥id_rsa.pub</strong>(默认生成在<code>c/Users/YourNames/.ssh/</code>)</li>
</ul>
<h5 id="2-将公钥id-rsa-pub的内容添加到Github"><a href="#2-将公钥id-rsa-pub的内容添加到Github" class="headerlink" title="(2)将公钥id_rsa.pub的内容添加到Github"></a>(2)将公钥id_rsa.pub的内容添加到Github</h5><p>(a)找到该文件,用记事本打开文件复制.如果找不到的话也可以用Git Bash直接打开.只需要输入以下指令:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cd ~/.ssh</span><br><span class="line">$ ls</span><br><span class="line">$ cat id_rsa.pub</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220430154016644.png"></p>
<p>(b)接下来,利用Ctrl+Insert复制公钥内容,进入Github主页,单击右上角头像进入Settings界面.</p>
<p>(c)选择<code>SSH and GPG keys</code>项,将公钥<code>id_rsa.pub</code>的内容粘贴到key当中,然后点击<code>Add SSH key</code>,如下图所示:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220430154631831.png"></p>
<p>(d)验证是否成功,可以在<code>Git Bash</code>用输入:</p>
<p><code>ssh -T git@github.com</code>来进行检验.</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220430154830022.png"></p>
<p>(第一次执行上述步骤的话可能会出现一句提示:Are you sure you want to continue connecting?,在这里选择yes就可以,正常也会出现上图的成功情况)</p>
<h4 id="2-提交文件"><a href="#2-提交文件" class="headerlink" title="2.提交文件"></a>2.提交文件</h4><p>这里可以参考关于Git操作的博客,总而言之提交文件有如下方式:</p>
<blockquote>
<p><strong>方式1:如果本地没有git仓库</strong>[实战](# (1)方式1实战)</p>
<ul>
<li>1.clone远程仓库</li>
<li>2.添加文件,commit并push到远程仓库.</li>
</ul>
<p><strong>方式2:如果本地已经有仓库,并且有commit操作:</strong>[实战](# (2)方法2实战)</p>
<ul>
<li>1.建立一个本地仓库并进入,init初始化</li>
<li>2.关联远程仓库</li>
<li>3.同步本地仓库与远程仓库</li>
<li>4.将文件添加提交到本地仓库,并push到远程仓库</li>
</ul>
</blockquote>
<h5 id="1-方式1实战"><a href="#1-方式1实战" class="headerlink" title="(1)方式1实战"></a>(1)方式1实战</h5><ul>
<li>首先,假设Github上已有了一个项目test,则我们需要打开<code>Git Bash</code>,找到仓库地址并<code>clone</code>下来,如:</li>
</ul>
<p><code>git clone https://github.com/hhlovesyy/test0818.git</code></p>
<ul>
<li><p><strong>测试环节:本地添加一个额外的文件test.txt</strong>,随便输入一些内容,然后在文件夹内打开<code>Git Bash</code>并输入<code>git status</code>查看状态:</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220430160000536.png"></li>
</ul>
</li>
<li><p>这是因为我们新建的文件没有被追踪,接下来就是熟悉的Git操作了(不了解的话可以看参考文章),输入以下内容:</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git add test.txt</span><br><span class="line">git commit -m &quot;commit test file&quot;</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>如果第一次提交的话会出现下述情况让输入用户名和邮箱,按参考文章步骤即可:</strong></p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220430160408252.png"></li>
</ul>
</li>
<li><p>完成后输入<code>git log</code>命令查看仓库提交日志</p>
</li>
<li><p>再输入一下<code>git status</code>查看仓库状态</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220430160720826.png"></li>
</ul>
</li>
<li><p>注意到此时远程主机的名字是mains,因此在这里使用push命令<code>git push origin mains</code>(<strong>一般没有特殊处理过的话这里应该是git push origin master</strong>)</p>
</li>
</ul>
<p>(如果第一次上传的话需要输入密码,验证成功后即可,此时刷新远端仓库应该就可以看到上传成功了,如果出现错误的话参考下图的解决方案:)</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220430161945951.png"></p>
<h5 id="2-方法2实战"><a href="#2-方法2实战" class="headerlink" title="(2)方法2实战"></a>(2)方法2实战</h5><ul>
<li><p>首先,新建一个本地仓库demo,然后用<code>git init</code>初始化仓库.接下来输入<code>git remote add origin xxx.git</code>关联到远程仓库.</p>
</li>
<li><p>输入<code>git pull origin mains(没有改过就是master)</code>,同步远程仓库和本地仓库,如下:</p>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220430161654621.png"></li>
</ul>
</li>
<li><p>此时回到本地仓库,会发现我们已经把远程仓库内容同步到了本地仓库,接下来就是基本同[方法1](# (1)方法1实战),使用<code>git add,git commit和git push origin master</code>命令,将本地仓库修改或添加的内容提交到远程仓库了.</p>
</li>
</ul>
<h3 id="四-购买域名-这一步可以跳过"><a href="#四-购买域名-这一步可以跳过" class="headerlink" title="四.购买域名(这一步可以跳过)"></a>四.购买域名(这一步可以跳过)</h3><h3 id="五-安装node-js和Hexo"><a href="#五-安装node-js和Hexo" class="headerlink" title="五.安装node.js和Hexo"></a>五.安装node.js和Hexo</h3><h4 id="1-安装node-js"><a href="#1-安装node-js" class="headerlink" title="1.安装node.js"></a>1.安装node.js</h4><p>下载网站如下:</p>
<p><a target="_blank" rel="noopener" href="https://nodejs.org/en/">Node.js (nodejs.org)</a></p>
<p>选择LTS版本进行安装(中间出现了需要打勾的地方可以选择打勾),安装之后可以打开cmd,输入下述指令检查版本(如果未能识别相关指令,<strong>注意看是否是由于没有配置环境变量导致的</strong>):</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">node -v</span><br><span class="line">npm -v</span><br></pre></td></tr></table></figure>

<p>过程当中可能需要配置一些与环境变量相关的内容,可以参考文章(<strong>建议阅读一下,否则可能会出现一些安装上的问题</strong>):<a target="_blank" rel="noopener" href="https://blog.csdn.net/antma/article/details/86104068">(11条消息) node.js 安装详细步骤教程_程序员老油条的博客-CSDN博客_node.js</a></p>
<p>(1)安装完nodejs并配置好环境变量之后,在安装的文件夹当中新建两个文件夹<code>node_cache</code>和<code>node_global</code>用来解决占用C盘的问题.</p>
<p><strong>注意,需要右键设置文件属性,将用户具有完全属性的权限,否则后续安装可能会出错.</strong></p>
<p>(2)打开cmd,输入下述指令:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm config set prefix &quot;D:nodejsnew\node_global&quot;</span><br><span class="line">npm config set cache &quot;D:nodejsnew\node_cache&quot;</span><br></pre></td></tr></table></figure>

<p>(要灵活根据自己的文件夹的路径进行修改)</p>
<p>(3)新建系统环境变量NODE_PATH,值为以下路径(<strong>注意,这一步要编辑系统变量,不是用户环境变量</strong>):</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220430164048730.png"></p>
<p>(4)<strong>编辑用户环境变量path,将其中对应的npm路径改为node_global的路径,如下</strong>:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220430164335520.png"></p>
<p>修改之后,进行测试:</p>
<p>(5)在cmd命令下执行<code>npm install webpack -g</code></p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220430164848682.png"></p>
<p>注意,<strong>如果有很多错误信息则可能是因为文件夹给的权限不够高,此时需要修改权限为完全控制</strong></p>
<h4 id="2-安装Hexo"><a href="#2-安装Hexo" class="headerlink" title="2.安装Hexo"></a>2.安装Hexo</h4><p>在安装Hexo之前,需要在Github上先创立一个仓库(<strong>注意,该仓库要是Public,并且该仓库要以.github.io结尾</strong>,名字是用户名).</p>
<h5 id="1-安装Hexo"><a href="#1-安装Hexo" class="headerlink" title="(1)安装Hexo"></a>(1)安装Hexo</h5><p>(a)首先,在本机上新建一个文件夹Blog,右键打开<code>Git Bash</code>,输入npm指令安装Hexo:</p>
<p><code> npm install -g hexo-cli</code></p>
<p>(b)安装完成后,输入<code>hexo init</code>命令初始化博客;</p>
<p>(c)接下来,输入<code>hexo g</code>静态部署;</p>
<p>(d)此时网页应该已经部署完成,输入<code>hexo s</code>命令可以查看:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220430165839224.png"></p>
<p>浏览器输入对应的网址((<a target="_blank" rel="noopener" href="http://localhost:4000/">http://localhost:4000</a>))</p>
<p>即可打开新部署的网页.效果如下(<strong>注意此时需要保持服务器的开启状态</strong>):</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220430165943730.png"></p>
<p>此时按下Ctrl+C即可停止运行服务器.</p>
<h5 id="2-将Hexo部署到Github"><a href="#2-将Hexo部署到Github" class="headerlink" title="(2)将Hexo部署到Github"></a>(2)将Hexo部署到Github</h5><p>回到Blog文件夹,用记事本打开<code>_config.yml</code>文件,下滑到最底部,添加如下内容:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: https://github.com/hhlovesyy/hhlovesyy.github.io.git  #你的仓库地址</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure>

<p>(注意命令中空格不要忘记)</p>
<h5 id="3-安装Git部署插件-重要"><a href="#3-安装Git部署插件-重要" class="headerlink" title="(3)安装Git部署插件(重要!)"></a>(3)安装Git部署插件(重要!)</h5><p>回到Blog文件夹并打开<code>Git Bash</code>,此时通过以下命令安装Git部署插件:</p>
<p><code>npm install hexo-deployer-git --save</code></p>
<p>接下来分别输入三条命令(<strong>每次更新博客的内容都要记得输入这个相关的内容</strong>):</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean   #清除缓存文件 db.json 和已生成的静态文件 public</span><br><span class="line">hexo g       #生成网站静态文件到默认设置的 public 文件夹(hexo generate 的缩写)</span><br><span class="line">hexo d       #自动生成网站静态文件，并部署到设定的仓库(hexo deploy 的缩写)</span><br></pre></td></tr></table></figure>



<h5 id="4-查看部署的结果"><a href="#4-查看部署的结果" class="headerlink" title="(4)查看部署的结果"></a>(4)查看部署的结果</h5><p>打开浏览器,输入((<a target="_blank" rel="noopener" href="http://xxx.github.io/">http://xxx.github.io</a>))</p>
<p>即可尝试访问(xxx为仓库前面的前缀,比如我是hhlovesyy),注意在输入网址测试的时候可能要保持服务器开启(<strong>在这里我输入的是hexo s开启服务器</strong>,不过经过后面的尝试实际上不开启也是可以的)</p>
<p>效果如下:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220430175352262.png"></p>
<h3 id="六-设置next主题"><a href="#六-设置next主题" class="headerlink" title="六.设置next主题"></a>六.设置next主题</h3><p>目前来说虽然博客有界面了,但是并不美观,因此这里我们相对其进行美化.</p>
<p>这里使用的是next来更改设置,步骤如下:</p>
<p>(1)大概博客根目录Blog,右键<code>Git Bash</code>,输入如下内容将next主题下载到<code>Blog/themes</code>:</p>
<p><code>git clone https://github.com/theme-next/hexo-theme-next themes/next</code></p>
<p>此时下载后的地址如下:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220430175928617.png"></p>
<p>打开根目录下的_config.yml(称为<strong>站点配置文件</strong>)，修改主题（<strong>注意冒号后都要有空格</strong>）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># Site</span><br><span class="line">title: ChrisZhang  #标题</span><br><span class="line">subtitle: &#x27;&#x27;</span><br><span class="line">description: 我的技美学习之路     #简介或者格言</span><br><span class="line">keywords:</span><br><span class="line">author: ChrisZhang     #作者</span><br><span class="line">language: zh-CN     #主题语言</span><br><span class="line">timezone: Asia/Shanghai    #中国的时区</span><br><span class="line"></span><br><span class="line"># Extensions</span><br><span class="line">## Plugins: https://hexo.io/plugins/</span><br><span class="line">## Themes: https://hexo.io/themes/</span><br><span class="line">theme: next   #主题改为next(不要忘了这个)</span><br></pre></td></tr></table></figure>

<p><strong>不要忘了修改主题为next,否则部署会一直出现问题(因为没有改为next配置)</strong></p>
<p>(2)修改主题,选择喜欢的主题(next主题有四种，如下图依次为Muse、Mist、Pisces、Gemini)</p>
<p>比如选择Gemini,则打开目录Blog&#x2F;themes&#x2F;next&#x2F;下的_config.yml（称为<strong>主题配置文件</strong>），只要将选的主题前的#删除就行了：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Schemes</span><br><span class="line">#scheme: Muse</span><br><span class="line">#scheme: Mist</span><br><span class="line">#scheme: Pisces</span><br><span class="line">scheme: Gemini    #选择这个主题</span><br></pre></td></tr></table></figure>



<p>(3)回到根目录,然后<code>Git Bash</code>,输入之前的那三条指令:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean   #清除缓存文件 db.json 和已生成的静态文件 public</span><br><span class="line">hexo g       #生成网站静态文件到默认设置的 public 文件夹(hexo generate 的缩写)</span><br><span class="line">hexo d       #自动生成网站静态文件，并部署到设定的仓库(hexo deploy 的缩写)</span><br></pre></td></tr></table></figure>

<p><strong>此时再登录网站,查看是否可以看到更新之后的结果</strong>.</p>
<p>如果不行的话,可以试一下作者的解决方案:</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220430180918640.png"></p>
<h3 id="七-额外的各种功能-根据喜好"><a href="#七-额外的各种功能-根据喜好" class="headerlink" title="七.额外的各种功能(根据喜好)"></a>七.额外的各种功能(根据喜好)</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/106060640">个人博客第8篇——优化主题（持续更新） - 知乎 (zhihu.com)</a></p>
<h4 id="1-设置菜单"><a href="#1-设置菜单" class="headerlink" title="1.设置菜单"></a>1.设置菜单</h4><p>打开配置文件themes&#x2F;next下的_config.yml，查找menu，将前面的#删除就行了。</p>
<p>结果如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: / || home                      #首页</span><br><span class="line">  archives: /archives/ || archive      #归档</span><br><span class="line">  categories: /categories/ || th       #分类</span><br><span class="line">  tags: /tags/ || tags                 #标签</span><br><span class="line">  about: /about/ || user               #关于</span><br><span class="line">  resources: /resources/ || download   #资源</span><br><span class="line">  #schedule: /schedule/ || calendar    #日历</span><br><span class="line">  #sitemap: /sitemap.xml || sitemap    #站点地图，供搜索引擎爬取</span><br><span class="line">  #commonweal: /404/ || heartbeat      #腾讯公益404</span><br></pre></td></tr></table></figure>

<p>(注:新版可能变成类似于<code>fafa-home</code>这种,其他的不用担心)</p>
<p>“||”前面的是目标链接，后面的是图标名称，next使用的图标全是<a href="https://link.zhihu.com/?target=http://www.fontawesome.com.cn/faicons/%23web-application">图标库 - Font Awesome 中文网</a>这一网站的，有想用的图标直接在font awesome上面找图标的名称就行。resources是自己后面添加的,如果有其他的需求也可以对应添加.</p>
<p><strong>新添加的菜单需要翻译中文,因此打开theme&#x2F;next&#x2F;languages&#x2F;zh-CN.yml,在menu下把对应的中文放进去</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: 首页</span><br><span class="line">  archives: 归档</span><br><span class="line">  categories: 分类</span><br><span class="line">  tags: 标签</span><br><span class="line">  about: 关于</span><br><span class="line">  resources: 资源</span><br><span class="line">  search: 搜索</span><br></pre></td></tr></table></figure>



<p>然后在根目录下打开<code>Git Bash</code>,输入如下代码:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo new page &quot;categories&quot;</span><br><span class="line">hexo new page &quot;tags&quot;</span><br><span class="line">hexo new page &quot;about&quot;</span><br><span class="line">hexo new page &quot;resources&quot;</span><br></pre></td></tr></table></figure>

<p>此时可以看到blog文件夹下的source文件夹中会生成上面的几个文件夹,每个对应的文件夹里面有一个<code>index.md</code>文件,修改内容如下(<strong>注意:要删掉原来所有的内容,加入下面的部分,不要保留一些typora的格式否则会自带一些编辑状态才能看到的内容,会导致网页生成错误</strong>):</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 分类</span><br><span class="line">date: 2022-04-30 22:07:08</span><br><span class="line">type: &quot;categories&quot;</span><br><span class="line">comments: false</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">title: 标签</span><br><span class="line">date: 2022-04-30 22:07:08</span><br><span class="line">type: &quot;tags&quot;</span><br><span class="line">comments: false</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">title: 关于</span><br><span class="line">date: 2022-04-30 22:07:08</span><br><span class="line">type: &quot;about&quot;</span><br><span class="line">comments: false</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">title: 资源</span><br><span class="line">date: 2022-04-30 22:07:08</span><br><span class="line">type: &quot;resources&quot;</span><br><span class="line">comments: false</span><br><span class="line">---</span><br></pre></td></tr></table></figure>

<p>注：如果有启用评论，默认页面带有评论。需要关闭的话，添加字段comments并将值设置为false。</p>
<h4 id="2-设置建站时间"><a href="#2-设置建站时间" class="headerlink" title="2.设置建站时间"></a>2.设置建站时间</h4><p>打开主题配置文件即themes&#x2F;next下的_config.yml，查找since：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">footer:</span><br><span class="line">  # Specify the date when the site was setup. If not defined, current year will be used.</span><br><span class="line">  since: 2022-04-30   #建站时间</span><br></pre></td></tr></table></figure>



<h4 id="3-设置头像"><a href="#3-设置头像" class="headerlink" title="3.设置头像"></a>3.设置头像</h4><p>打开主题配置文件即themes&#x2F;next下的_config.yml，查找avatar，url后是图片的链接地址：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Sidebar Avatar</span><br><span class="line">avatar:</span><br><span class="line">  # Replace the default image and set the url here.</span><br><span class="line">  url: /images/avatar.gif   #图片的位置，也可以是http://xxx.com/avatar.png</span><br><span class="line">  # If true, the avatar will be dispalyed in circle.</span><br><span class="line">  rounded: true   #头像展示在圈里</span><br><span class="line">  # If true, the avatar will be rotated with the cursor.</span><br><span class="line">  rotated: false  #头像随光标旋转</span><br></pre></td></tr></table></figure>

<p>然后将你要的头像图片复制到themes&#x2F;next&#x2F;source&#x2F;images里，重命名为avatar.png。</p>
<h4 id="4-网站图标设置"><a href="#4-网站图标设置" class="headerlink" title="4.网站图标设置"></a>4.网站图标设置</h4><p>暂未进行设置</p>
<h4 id="5-设置动态背景"><a href="#5-设置动态背景" class="headerlink" title="5.设置动态背景"></a>5.设置动态背景</h4><ul>
<li>这一条目前没有很大的需求(希望个人的博客可以保持简约直白的风格),因此这里不做处理</li>
</ul>
<p>有需求的话可以参考博客<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/106060640">个人博客第8篇——优化主题（持续更新） - 知乎 (zhihu.com)</a></p>
<h4 id="6-设置背景图片"><a href="#6-设置背景图片" class="headerlink" title="6.设置背景图片"></a>6.设置背景图片</h4><p>暂无需求</p>
<h4 id="7-主页文章添加阴影效果"><a href="#7-主页文章添加阴影效果" class="headerlink" title="7.主页文章添加阴影效果"></a>7.主页文章添加阴影效果</h4><p>暂无需求</p>
<h4 id="8-添加顶部加载条"><a href="#8-添加顶部加载条" class="headerlink" title="8.添加顶部加载条"></a>8.添加顶部加载条</h4><p>暂无需求</p>
<h4 id="9-设置预览摘要"><a href="#9-设置预览摘要" class="headerlink" title="9.设置预览摘要"></a>9.设置预览摘要</h4><p>next（v7.7.1）已经没有如下代码：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">auto_excerpt:</span><br><span class="line">  enable: true</span><br><span class="line">  length: 150</span><br></pre></td></tr></table></figure>

<p>所以不需要设置，只要我们在文章中插入 <code>&lt;!-- more --&gt;</code>，该标签之上的是摘要，之后的内容不可见，需点击全文阅读按钮：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- more --&gt;</span><br></pre></td></tr></table></figure>



<h4 id="10-设置侧边栏显示效果"><a href="#10-设置侧边栏显示效果" class="headerlink" title="10.设置侧边栏显示效果"></a>10.设置侧边栏显示效果</h4><p>打开<strong>主题配置文件</strong>即themes&#x2F;next下的_config.yml，找到Sidebar Settings，设置：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sidebar:</span><br><span class="line">  # Sidebar Position. #设置侧边栏位置</span><br><span class="line">  position: left</span><br><span class="line">  #position: right</span><br><span class="line"></span><br><span class="line">  #  - post    默认显示模式</span><br><span class="line">  #  - always  一直显示</span><br><span class="line">  #  - hide    初始隐藏</span><br><span class="line">  #  - remove  移除侧边栏</span><br><span class="line">  display: post</span><br></pre></td></tr></table></figure>



<h4 id="11-侧边栏推荐阅读"><a href="#11-侧边栏推荐阅读" class="headerlink" title="11.侧边栏推荐阅读"></a>11.侧边栏推荐阅读</h4><p>这里可以放置一些自己在其他平台所写的文章(比如知乎,bilibili等)</p>
<p>打开<strong>主题配置文件</strong>即themes&#x2F;next下的_config.yml，搜索links（里面写你想要的链接）：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">links_settings:</span><br><span class="line">  icon: link</span><br><span class="line">  title: 链接网站  #修改名称</span><br><span class="line"></span><br><span class="line">links:</span><br><span class="line">  #Title: http://yoursite.com</span><br><span class="line">  百度: https://baidu.com</span><br></pre></td></tr></table></figure>



<h4 id="12-添加社交链接"><a href="#12-添加社交链接" class="headerlink" title="12.添加社交链接"></a>12.添加社交链接</h4><p>同样也是打开主题配置文件,搜索social,添加个人常使用的社交平台.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">social:</span><br><span class="line">  GitHub: https://github.com/hhlovesyy || fab fa-github</span><br><span class="line">  E-Mail: mailto:1596944152@qq.com || fa fa-envelope</span><br><span class="line">  知乎: https://www.zhihu.com/people/liang-dao-men || fa fa-quora</span><br></pre></td></tr></table></figure>

<p>设置完11,12两个步骤之后,就可以在<code>资源</code>一栏的<code>站点概览</code>中浏览到个人相关的链接了.</p>
<h4 id="13-设置博文内的链接为蓝色"><a href="#13-设置博文内的链接为蓝色" class="headerlink" title="13.设置博文内的链接为蓝色"></a>13.设置博文内的链接为蓝色</h4><p>打开<code>themes/next/source/css/_common/components/post/post.styl</code>文件，将下面的代码复制到文件最后：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.post-body p a&#123;</span><br><span class="line">     color: #0593d3;</span><br><span class="line">     border-bottom: none;</span><br><span class="line">     &amp;:hover &#123;</span><br><span class="line">       color: #0477ab;</span><br><span class="line">       text-decoration: underline;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>



<p>到这里可以重新部署一下,看看效果怎么样.</p>
<h4 id="14-设置图床"><a href="#14-设置图床" class="headerlink" title="14.设置图床"></a>14.设置图床</h4><p>这里有一篇很适合阅读的图床搭建教程：</p>
<p><a target="_blank" rel="noopener" href="https://www.itbob.cn/article/005/">Github + jsDelivr + PicGo 打造稳定快速、高效免费图床 | ITBOB’S BLOG</a></p>
<p>按照上面的博客，PicGo设置如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220501204637601.png" alt="image-20220501204637601"></p>
<p>在设置完图床之后，为了方便起见最好把Typora编辑器和PicGo软件进行绑定，步骤如下：</p>
<p>文件-&gt;偏好设置-&gt;图像，然后参考下图修改：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/ITRHX-PIC/image-20220501204543214.png" alt="image-20220501204543214"></p>
<h4 id="15-给文章添加分类和标签"><a href="#15-给文章添加分类和标签" class="headerlink" title="15.给文章添加分类和标签"></a>15.给文章添加分类和标签</h4><p>参考教程:</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/348131730">hexo博客中tags与categories用法 - 知乎 (zhihu.com)</a>(优先看这个)</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/50787870">Hexo+Github博客教程：03添加分类 - 知乎 (zhihu.com)</a></p>
<p>方便起见，每次想要新写一篇文章的时候可以使用hexo new 指令来方便快捷地新增文章，同时文章会自动绑定如下的格式：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502163514571.png" alt="image-20220502163514571"></p>
<p><img src="https://cdn.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502163630699.png" alt="image-20220502163630699"></p>
<h4 id="16-使得博客渲染支持LaTeX语法"><a href="#16-使得博客渲染支持LaTeX语法" class="headerlink" title="16.使得博客渲染支持LaTeX语法"></a>16.使得博客渲染支持LaTeX语法</h4><p>参考文章:<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43318626/article/details/89407031">(11条消息) 在HEXO博客中使用LaTeX公式的简单方法_Loy_Fan的博客-CSDN博客_hexo latex</a></p>
<p>按照上述文章的步骤来完成即可,注意在Windows操作系统下删除操作并不需要sudo,所以对应指令部分只需要把sudo删掉即可,最后测试LaTeX渲染成功.</p>
<h4 id="17-Hexo-NexT设置代码块的复制功能"><a href="#17-Hexo-NexT设置代码块的复制功能" class="headerlink" title="17.Hexo NexT设置代码块的复制功能"></a>17.Hexo NexT设置代码块的复制功能</h4><p>很多时候,我们希望能让读者或是自己后续复习的时候可以复制博客中代码块,因此需要添加一个复制代码块的功能,方法如下:</p>
<p>在next主题配置文件中启用<code>copy_button</code>即可</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/30/Leetcode%200401/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="ChrisZhang">
      <meta itemprop="description" content="我的技美学习之路">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ChrisZhang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/30/Leetcode%200401/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-04-30 18:13:50" itemprop="dateCreated datePublished" datetime="2022-04-30T18:13:50+08:00">2022-04-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-04-04 21:30:38" itemprop="dateModified" datetime="2022-04-04T21:30:38+08:00">2022-04-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="Leetcode-0401"><a href="#Leetcode-0401" class="headerlink" title="Leetcode 0401"></a>Leetcode 0401</h3><h4 id="1-leetcode-59-螺旋矩阵Ⅱ"><a href="#1-leetcode-59-螺旋矩阵Ⅱ" class="headerlink" title="1.leetcode 59 螺旋矩阵Ⅱ"></a>1.leetcode 59 螺旋矩阵Ⅱ</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/spiral-matrix-ii/">59. 螺旋矩阵 II - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">generateMatrix</span>(<span class="type">int</span> n) &#123;</span><br><span class="line">        <span class="type">int</span> l=<span class="number">0</span>,r=n<span class="number">-1</span>,t=<span class="number">0</span>,b=n<span class="number">-1</span>;</span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">res</span>(n,<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n,<span class="number">0</span>));</span><br><span class="line">        <span class="type">int</span> cnt=<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(cnt&lt;=n*n)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i=l,j=t;i&lt;=r;i++)</span><br><span class="line">                res[j][i]=cnt++;</span><br><span class="line">            t++;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i=t,j=r;i&lt;=b;i++)</span><br><span class="line">                res[i][j]=cnt++;</span><br><span class="line">            r--;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i=r,j=b;i&gt;=l;i--)</span><br><span class="line">                res[j][i]=cnt++;</span><br><span class="line">            b--;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i=b,j=l;i&gt;=t;i--)</span><br><span class="line">                res[i][j]=cnt++;</span><br><span class="line">            l++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="2-剑指offer-03-数组中重复的数字"><a href="#2-剑指offer-03-数组中重复的数字" class="headerlink" title="2.剑指offer 03 数组中重复的数字"></a>2.剑指offer 03 数组中重复的数字</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof/">剑指 Offer 03. 数组中重复的数字 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<ul>
<li>方法1:哈希</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">findRepeatNumber</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        unordered_map&lt;<span class="type">int</span>,<span class="type">bool</span>&gt; tempres;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;nums.<span class="built_in">size</span>();i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(tempres[nums[i]]==<span class="literal">true</span>)</span><br><span class="line">                <span class="keyword">return</span> nums[i];</span><br><span class="line">            tempres[nums[i]]=<span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<ul>
<li>方法2:原地交换</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">findRepeatNumber</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//原地交换</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;nums.<span class="built_in">size</span>();)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(nums[i]==i)</span><br><span class="line">                i++;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(nums[nums[i]]==nums[i])</span><br><span class="line">                    <span class="keyword">return</span> nums[i];</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    <span class="built_in">swap</span>(nums[i],nums[nums[i]]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="3-剑指offer-04-二维数组中的查找"><a href="#3-剑指offer-04-二维数组中的查找" class="headerlink" title="3.剑指offer 04 二维数组中的查找"></a>3.剑指offer 04 二维数组中的查找</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/er-wei-shu-zu-zhong-de-cha-zhao-lcof/">剑指 Offer 04. 二维数组中的查找 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<ul>
<li>注意,这题不要跟leetcode 74 搜索二维矩阵弄混,<a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/search-a-2d-matrix/">74. 搜索二维矩阵 - 力扣（LeetCode） (leetcode-cn.com)</a></li>
</ul>
<p>(本题没有规定每行的第一个整数大于前一行的最后一个整数,而74题则规定了这件事)</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">findNumberIn2DArray</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; matrix, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n=matrix.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(n==<span class="number">0</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="type">int</span> m=matrix[<span class="number">0</span>].<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(m==<span class="number">0</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="type">int</span> l=<span class="number">0</span>,b=n<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">while</span>(l&gt;=<span class="number">0</span>&amp;&amp;l&lt;m&amp;&amp;b&gt;=<span class="number">0</span>&amp;&amp;b&lt;n)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(matrix[b][l]&lt;target)</span><br><span class="line">                l++;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(matrix[b][l]&gt;target)</span><br><span class="line">                b--;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(matrix[b][l]==target)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>理论上用while也可以,但是非常慢(<strong>强烈不推荐</strong>)</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">findNumberIn2DArray</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; matrix, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n=matrix.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(n==<span class="number">0</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="type">int</span> m=matrix[<span class="number">0</span>].<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(m==<span class="number">0</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="type">int</span> l=<span class="number">0</span>,b=n<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">while</span>(l&gt;=<span class="number">0</span>&amp;&amp;l&lt;m&amp;&amp;b&gt;=<span class="number">0</span>&amp;&amp;b&lt;n)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">while</span>(l&gt;=<span class="number">0</span>&amp;&amp;l&lt;m&amp;&amp;b&gt;=<span class="number">0</span>&amp;&amp;b&lt;n&amp;&amp;matrix[b][l]&lt;target)</span><br><span class="line">                l++;</span><br><span class="line">            <span class="keyword">while</span>((l&gt;=<span class="number">0</span>&amp;&amp;l&lt;m&amp;&amp;b&gt;=<span class="number">0</span>&amp;&amp;b&lt;n)&amp;&amp;matrix[b][l]&gt;target)</span><br><span class="line">                b--;</span><br><span class="line">            <span class="keyword">if</span>(!(l&gt;=<span class="number">0</span>&amp;&amp;l&lt;m&amp;&amp;b&gt;=<span class="number">0</span>&amp;&amp;b&lt;n))<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">if</span>(matrix[b][l]==target)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>





<h4 id="4-leetcode-74-搜索二维矩阵"><a href="#4-leetcode-74-搜索二维矩阵" class="headerlink" title="4.leetcode 74 搜索二维矩阵"></a>4.leetcode 74 搜索二维矩阵</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/search-a-2d-matrix/">74. 搜索二维矩阵 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">searchMatrix</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; matrix, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//两次二分法</span></span><br><span class="line">        <span class="type">int</span> n=matrix.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(n==<span class="number">0</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="type">int</span> m=matrix[<span class="number">0</span>].<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(m==<span class="number">0</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="type">int</span> l=<span class="number">0</span>,r=n<span class="number">-1</span>;</span><br><span class="line">        <span class="comment">//先找到在哪一行</span></span><br><span class="line">        <span class="keyword">if</span>(target&lt;matrix[<span class="number">0</span>][<span class="number">0</span>]) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">while</span>(l&lt;r)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> mid=(l+r+<span class="number">1</span>)/<span class="number">2</span>;</span><br><span class="line">            <span class="comment">//找到最后一个小于等于目标值的</span></span><br><span class="line">            <span class="keyword">if</span>(matrix[mid][<span class="number">0</span>]&gt;target)</span><br><span class="line">                r=mid<span class="number">-1</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                l=mid;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> row=l;</span><br><span class="line">        <span class="comment">//cout&lt;&lt;row&lt;&lt;endl;</span></span><br><span class="line">        l=<span class="number">0</span>,r=m<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">while</span>(l&lt;r)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">//找到第一个&gt;=target的值</span></span><br><span class="line">            <span class="type">int</span> mid=(l+r)/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(matrix[row][mid]&lt;target)</span><br><span class="line">                l=mid+<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span>   </span><br><span class="line">                r=mid;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(matrix[row][l]==target)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="5-leetcode-66-加一"><a href="#5-leetcode-66-加一" class="headerlink" title="5.leetcode 66 加一"></a>5.leetcode 66 加一</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/plus-one/">66. 加一 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<ul>
<li>通过这道题目可以复习一下大整数的加法,后续也可以看看y总的大数高精度篇.</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">plusOne</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; digits)</span> </span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        vector&lt;<span class="type">int</span>&gt; res;</span><br><span class="line">        <span class="type">int</span> num=<span class="number">0</span>;<span class="comment">//表示溢出的数字</span></span><br><span class="line">        digits[digits.<span class="built_in">size</span>()<span class="number">-1</span>]+=<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=digits.<span class="built_in">size</span>()<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(digits[i]+num&gt;=<span class="number">10</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                digits[i]=(digits[i]+num)<span class="number">-10</span>;</span><br><span class="line">                res.<span class="built_in">push_back</span>(digits[i]);</span><br><span class="line">                num=<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                res.<span class="built_in">push_back</span>(digits[i]+num);</span><br><span class="line">                num=<span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(num==<span class="number">1</span>) </span><br><span class="line">            res.<span class="built_in">push_back</span>(num);</span><br><span class="line">        <span class="built_in">reverse</span>(res.<span class="built_in">begin</span>(),res.<span class="built_in">end</span>());</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="6-leetcode-661-图片平滑器"><a href="#6-leetcode-661-图片平滑器" class="headerlink" title="6.leetcode 661 图片平滑器"></a>6.leetcode 661 图片平滑器</h4><p>(<strong>复习图像处理的一个知识点,就是卷积的时候不是对原图实时处理的(比如一个像素处理完,下一个像素的卷积核中的元素是未处理的,见下</strong>))</p>
<p>![image-20220401162041020](Leetcode 0401.assets&#x2F;image-20220401162041020.png)</p>
<p>(这里卷积后的10不作为下次卷积中卷积核里面的元素,下次卷积时卷积核里的元素<strong>还是原来图像的</strong>)</p>
<p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/image-smoother/">661. 图片平滑器 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> dx[<span class="number">9</span>]=&#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">1</span>,<span class="number">-1</span>,<span class="number">1</span>&#125;;</span><br><span class="line">    <span class="type">int</span> dy[<span class="number">9</span>]=&#123;<span class="number">0</span>,<span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">1</span>&#125;;</span><br><span class="line"></span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">imageSmoother</span>(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; img) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> m=img.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> n=img[<span class="number">0</span>].<span class="built_in">size</span>();</span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">res</span>(m,<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n,<span class="number">0</span>));</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;m;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;n;j++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="type">int</span> sum=<span class="number">0</span>;</span><br><span class="line">                <span class="type">int</span> cnt=<span class="number">0</span>;</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> k=<span class="number">0</span>;k&lt;<span class="number">9</span>;k++)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="type">int</span> tx=i+dx[k];</span><br><span class="line">                    <span class="type">int</span> ty=j+dy[k];</span><br><span class="line">                    <span class="keyword">if</span>(tx&gt;=<span class="number">0</span>&amp;&amp;tx&lt;m&amp;&amp;ty&gt;=<span class="number">0</span>&amp;&amp;ty&lt;n) <span class="comment">//表示可以算到平均值当中的</span></span><br><span class="line">                    &#123;</span><br><span class="line">                        sum+=img[tx][ty];</span><br><span class="line">                        cnt++;</span><br><span class="line">                    &#125;  </span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//cout&lt;&lt;sum&lt;&lt;cnt&lt;&lt;endl;</span></span><br><span class="line">                res[i][j]=(sum/cnt);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="7-leetcode-350-两个数组的交集Ⅱ"><a href="#7-leetcode-350-两个数组的交集Ⅱ" class="headerlink" title="7.leetcode 350 两个数组的交集Ⅱ"></a>7.leetcode 350 两个数组的交集Ⅱ</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/intersection-of-two-arrays-ii/">350. 两个数组的交集 II - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<ul>
<li>(1)纯哈希做法(两个数组,然后比对)</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">intersect</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums1, vector&lt;<span class="type">int</span>&gt;&amp; nums2)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//使用哈希表</span></span><br><span class="line">        <span class="type">int</span> map1[<span class="number">1010</span>]=&#123;<span class="number">0</span>&#125;;</span><br><span class="line">        <span class="type">int</span> map2[<span class="number">1010</span>]=&#123;<span class="number">0</span>&#125;;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;nums1.<span class="built_in">size</span>();i++)</span><br><span class="line">        &#123;</span><br><span class="line">            map1[nums1[i]]++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;nums2.<span class="built_in">size</span>();i++)</span><br><span class="line">        &#123;</span><br><span class="line">            map2[nums2[i]]++;</span><br><span class="line">        &#125;</span><br><span class="line">        vector&lt;<span class="type">int</span>&gt; res;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;<span class="number">1010</span>;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> cnt=<span class="built_in">min</span>(map1[i],map2[i]);</span><br><span class="line">            <span class="keyword">if</span>(cnt&gt;<span class="number">0</span>)</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;cnt;j++)</span><br><span class="line">                    res.<span class="built_in">push_back</span>(i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="8-leetcode-349-两个数组的交集"><a href="#8-leetcode-349-两个数组的交集" class="headerlink" title="8.leetcode 349 两个数组的交集"></a>8.leetcode 349 两个数组的交集</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/intersection-of-two-arrays/submissions/">349. 两个数组的交集 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">intersection</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums1, vector&lt;<span class="type">int</span>&gt;&amp; nums2)</span> </span>&#123;</span><br><span class="line">        vector&lt;<span class="type">int</span>&gt; res;</span><br><span class="line">        <span class="type">int</span> num1[<span class="number">1010</span>]=&#123;<span class="number">0</span>&#125;;</span><br><span class="line">        <span class="type">int</span> num2[<span class="number">1010</span>]=&#123;<span class="number">0</span>&#125;;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;nums1.<span class="built_in">size</span>();i++)</span><br><span class="line">            num1[nums1[i]]++;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;nums2.<span class="built_in">size</span>();i++)</span><br><span class="line">            num2[nums2[i]]++;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;<span class="number">1010</span>;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> cnt=<span class="built_in">min</span>(num1[i],num2[i]);</span><br><span class="line">            <span class="keyword">if</span>(cnt&gt;<span class="number">0</span>)</span><br><span class="line">                res.<span class="built_in">emplace_back</span>(i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="9-leetcode-434-字符串中的单词数"><a href="#9-leetcode-434-字符串中的单词数" class="headerlink" title="9.leetcode 434 字符串中的单词数"></a>9.leetcode 434 字符串中的单词数</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">countSegments</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> cnt=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(s.<span class="built_in">empty</span>()) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;s.<span class="built_in">size</span>()<span class="number">-1</span>;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">//什么样的算一个单词?</span></span><br><span class="line">            <span class="keyword">if</span>(s[i]!=<span class="string">&#x27; &#x27;</span>&amp;&amp;s[i+<span class="number">1</span>]==<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                cnt++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(s[s.<span class="built_in">size</span>()<span class="number">-1</span>]!=<span class="string">&#x27; &#x27;</span>) cnt++;</span><br><span class="line">        <span class="keyword">return</span> cnt;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="10-leetcode-1732-找到最高海拔"><a href="#10-leetcode-1732-找到最高海拔" class="headerlink" title="10.leetcode 1732 找到最高海拔"></a>10.leetcode 1732 找到最高海拔</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/find-the-highest-altitude/">1732. 找到最高海拔 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<ul>
<li>解法1:直接遍历数组,然后求和求解,注意数组下标的问题</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">largestAltitude</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; gain)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> maxres=<span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> temp=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=gain.<span class="built_in">size</span>();i++)</span><br><span class="line">        &#123;</span><br><span class="line">            temp+=gain[i<span class="number">-1</span>];</span><br><span class="line">            maxres=<span class="built_in">max</span>(maxres,temp);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> maxres;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="11-leetcode-344-反转字符串"><a href="#11-leetcode-344-反转字符串" class="headerlink" title="11.leetcode 344 反转字符串"></a>11.leetcode 344 反转字符串</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/reverse-string/">344. 反转字符串 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">reverseString</span><span class="params">(vector&lt;<span class="type">char</span>&gt;&amp; s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> l=<span class="number">0</span>,r=s.<span class="built_in">size</span>()<span class="number">-1</span>;l&lt;=r;) <span class="built_in">swap</span>(s[l++],s[r--]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>(水题)</p>
<h4 id="12-leetcode-206-反转链表"><a href="#12-leetcode-206-反转链表" class="headerlink" title="12.leetcode 206 反转链表"></a>12.leetcode 206 反转链表</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/reverse-linked-list/">206. 反转链表 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"> * struct ListNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     ListNode *next;</span></span><br><span class="line"><span class="comment"> *     ListNode() : val(0), next(nullptr) &#123;&#125;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x) : val(x), next(nullptr) &#123;&#125;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x, ListNode *next) : val(x), next(next) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">reverseList</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//反转链表的题目</span></span><br><span class="line">        <span class="comment">//来个虚拟头</span></span><br><span class="line">        <span class="keyword">if</span>(head==<span class="literal">nullptr</span>) <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        ListNode* pre=<span class="literal">nullptr</span>;</span><br><span class="line">        ListNode *cur=head;</span><br><span class="line">        ListNode *nextnode=head;</span><br><span class="line">        <span class="keyword">while</span>(cur)</span><br><span class="line">        &#123;</span><br><span class="line">            nextnode=cur-&gt;next;</span><br><span class="line">            cur-&gt;next=pre;</span><br><span class="line">            pre=cur;</span><br><span class="line">            cur=nextnode;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> pre;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="13-leetcode-19-删除链表的倒数第N个结点"><a href="#13-leetcode-19-删除链表的倒数第N个结点" class="headerlink" title="13.leetcode 19 删除链表的倒数第N个结点"></a>13.leetcode 19 删除链表的倒数第N个结点</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/remove-nth-node-from-end-of-list/">19. 删除链表的倒数第 N 个结点 - 力扣（LeetCode） (leetcode-cn.com)</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/30/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="ChrisZhang">
      <meta itemprop="description" content="我的技美学习之路">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ChrisZhang">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/30/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-04-30 16:56:59 / 修改时间：18:15:27" itemprop="dateCreated datePublished" datetime="2022-04-30T16:56:59+08:00">2022-04-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
<h3 id="Leetcode-0401"><a href="#Leetcode-0401" class="headerlink" title="Leetcode 0401"></a>Leetcode 0401</h3><h4 id="1-leetcode-59-螺旋矩阵Ⅱ"><a href="#1-leetcode-59-螺旋矩阵Ⅱ" class="headerlink" title="1.leetcode 59 螺旋矩阵Ⅱ"></a>1.leetcode 59 螺旋矩阵Ⅱ</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/spiral-matrix-ii/">59. 螺旋矩阵 II - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">generateMatrix</span>(<span class="type">int</span> n) &#123;</span><br><span class="line">        <span class="type">int</span> l=<span class="number">0</span>,r=n<span class="number">-1</span>,t=<span class="number">0</span>,b=n<span class="number">-1</span>;</span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">res</span>(n,<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n,<span class="number">0</span>));</span><br><span class="line">        <span class="type">int</span> cnt=<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(cnt&lt;=n*n)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i=l,j=t;i&lt;=r;i++)</span><br><span class="line">                res[j][i]=cnt++;</span><br><span class="line">            t++;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i=t,j=r;i&lt;=b;i++)</span><br><span class="line">                res[i][j]=cnt++;</span><br><span class="line">            r--;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i=r,j=b;i&gt;=l;i--)</span><br><span class="line">                res[j][i]=cnt++;</span><br><span class="line">            b--;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i=b,j=l;i&gt;=t;i--)</span><br><span class="line">                res[i][j]=cnt++;</span><br><span class="line">            l++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="2-剑指offer-03-数组中重复的数字"><a href="#2-剑指offer-03-数组中重复的数字" class="headerlink" title="2.剑指offer 03 数组中重复的数字"></a>2.剑指offer 03 数组中重复的数字</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof/">剑指 Offer 03. 数组中重复的数字 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<ul>
<li>方法1:哈希</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">findRepeatNumber</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        unordered_map&lt;<span class="type">int</span>,<span class="type">bool</span>&gt; tempres;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;nums.<span class="built_in">size</span>();i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(tempres[nums[i]]==<span class="literal">true</span>)</span><br><span class="line">                <span class="keyword">return</span> nums[i];</span><br><span class="line">            tempres[nums[i]]=<span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<ul>
<li>方法2:原地交换</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">findRepeatNumber</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//原地交换</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;nums.<span class="built_in">size</span>();)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(nums[i]==i)</span><br><span class="line">                i++;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(nums[nums[i]]==nums[i])</span><br><span class="line">                    <span class="keyword">return</span> nums[i];</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    <span class="built_in">swap</span>(nums[i],nums[nums[i]]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="3-剑指offer-04-二维数组中的查找"><a href="#3-剑指offer-04-二维数组中的查找" class="headerlink" title="3.剑指offer 04 二维数组中的查找"></a>3.剑指offer 04 二维数组中的查找</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/er-wei-shu-zu-zhong-de-cha-zhao-lcof/">剑指 Offer 04. 二维数组中的查找 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<ul>
<li>注意,这题不要跟leetcode 74 搜索二维矩阵弄混,<a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/search-a-2d-matrix/">74. 搜索二维矩阵 - 力扣（LeetCode） (leetcode-cn.com)</a></li>
</ul>
<p>(本题没有规定每行的第一个整数大于前一行的最后一个整数,而74题则规定了这件事)</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">findNumberIn2DArray</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; matrix, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n=matrix.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(n==<span class="number">0</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="type">int</span> m=matrix[<span class="number">0</span>].<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(m==<span class="number">0</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="type">int</span> l=<span class="number">0</span>,b=n<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">while</span>(l&gt;=<span class="number">0</span>&amp;&amp;l&lt;m&amp;&amp;b&gt;=<span class="number">0</span>&amp;&amp;b&lt;n)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(matrix[b][l]&lt;target)</span><br><span class="line">                l++;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(matrix[b][l]&gt;target)</span><br><span class="line">                b--;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(matrix[b][l]==target)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>理论上用while也可以,但是非常慢(<strong>强烈不推荐</strong>)</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">findNumberIn2DArray</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; matrix, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n=matrix.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(n==<span class="number">0</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="type">int</span> m=matrix[<span class="number">0</span>].<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(m==<span class="number">0</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="type">int</span> l=<span class="number">0</span>,b=n<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">while</span>(l&gt;=<span class="number">0</span>&amp;&amp;l&lt;m&amp;&amp;b&gt;=<span class="number">0</span>&amp;&amp;b&lt;n)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">while</span>(l&gt;=<span class="number">0</span>&amp;&amp;l&lt;m&amp;&amp;b&gt;=<span class="number">0</span>&amp;&amp;b&lt;n&amp;&amp;matrix[b][l]&lt;target)</span><br><span class="line">                l++;</span><br><span class="line">            <span class="keyword">while</span>((l&gt;=<span class="number">0</span>&amp;&amp;l&lt;m&amp;&amp;b&gt;=<span class="number">0</span>&amp;&amp;b&lt;n)&amp;&amp;matrix[b][l]&gt;target)</span><br><span class="line">                b--;</span><br><span class="line">            <span class="keyword">if</span>(!(l&gt;=<span class="number">0</span>&amp;&amp;l&lt;m&amp;&amp;b&gt;=<span class="number">0</span>&amp;&amp;b&lt;n))<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">if</span>(matrix[b][l]==target)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>





<h4 id="4-leetcode-74-搜索二维矩阵"><a href="#4-leetcode-74-搜索二维矩阵" class="headerlink" title="4.leetcode 74 搜索二维矩阵"></a>4.leetcode 74 搜索二维矩阵</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/search-a-2d-matrix/">74. 搜索二维矩阵 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">searchMatrix</span><span class="params">(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; matrix, <span class="type">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//两次二分法</span></span><br><span class="line">        <span class="type">int</span> n=matrix.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(n==<span class="number">0</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="type">int</span> m=matrix[<span class="number">0</span>].<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span>(m==<span class="number">0</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="type">int</span> l=<span class="number">0</span>,r=n<span class="number">-1</span>;</span><br><span class="line">        <span class="comment">//先找到在哪一行</span></span><br><span class="line">        <span class="keyword">if</span>(target&lt;matrix[<span class="number">0</span>][<span class="number">0</span>]) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">while</span>(l&lt;r)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> mid=(l+r+<span class="number">1</span>)/<span class="number">2</span>;</span><br><span class="line">            <span class="comment">//找到最后一个小于等于目标值的</span></span><br><span class="line">            <span class="keyword">if</span>(matrix[mid][<span class="number">0</span>]&gt;target)</span><br><span class="line">                r=mid<span class="number">-1</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                l=mid;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> row=l;</span><br><span class="line">        <span class="comment">//cout&lt;&lt;row&lt;&lt;endl;</span></span><br><span class="line">        l=<span class="number">0</span>,r=m<span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">while</span>(l&lt;r)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">//找到第一个&gt;=target的值</span></span><br><span class="line">            <span class="type">int</span> mid=(l+r)/<span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(matrix[row][mid]&lt;target)</span><br><span class="line">                l=mid+<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span>   </span><br><span class="line">                r=mid;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(matrix[row][l]==target)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="5-leetcode-66-加一"><a href="#5-leetcode-66-加一" class="headerlink" title="5.leetcode 66 加一"></a>5.leetcode 66 加一</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/plus-one/">66. 加一 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<ul>
<li>通过这道题目可以复习一下大整数的加法,后续也可以看看y总的大数高精度篇.</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">plusOne</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; digits)</span> </span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        vector&lt;<span class="type">int</span>&gt; res;</span><br><span class="line">        <span class="type">int</span> num=<span class="number">0</span>;<span class="comment">//表示溢出的数字</span></span><br><span class="line">        digits[digits.<span class="built_in">size</span>()<span class="number">-1</span>]+=<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=digits.<span class="built_in">size</span>()<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(digits[i]+num&gt;=<span class="number">10</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                digits[i]=(digits[i]+num)<span class="number">-10</span>;</span><br><span class="line">                res.<span class="built_in">push_back</span>(digits[i]);</span><br><span class="line">                num=<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                res.<span class="built_in">push_back</span>(digits[i]+num);</span><br><span class="line">                num=<span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(num==<span class="number">1</span>) </span><br><span class="line">            res.<span class="built_in">push_back</span>(num);</span><br><span class="line">        <span class="built_in">reverse</span>(res.<span class="built_in">begin</span>(),res.<span class="built_in">end</span>());</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="6-leetcode-661-图片平滑器"><a href="#6-leetcode-661-图片平滑器" class="headerlink" title="6.leetcode 661 图片平滑器"></a>6.leetcode 661 图片平滑器</h4><p>(<strong>复习图像处理的一个知识点,就是卷积的时候不是对原图实时处理的(比如一个像素处理完,下一个像素的卷积核中的元素是未处理的,见下</strong>))</p>
<p>![image-20220401162041020](Leetcode 0401.assets&#x2F;image-20220401162041020.png)</p>
<p>(这里卷积后的10不作为下次卷积中卷积核里面的元素,下次卷积时卷积核里的元素<strong>还是原来图像的</strong>)</p>
<p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/image-smoother/">661. 图片平滑器 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> dx[<span class="number">9</span>]=&#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">1</span>,<span class="number">-1</span>,<span class="number">1</span>&#125;;</span><br><span class="line">    <span class="type">int</span> dy[<span class="number">9</span>]=&#123;<span class="number">0</span>,<span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">1</span>&#125;;</span><br><span class="line"></span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">imageSmoother</span>(vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp; img) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> m=img.<span class="built_in">size</span>();</span><br><span class="line">        <span class="type">int</span> n=img[<span class="number">0</span>].<span class="built_in">size</span>();</span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">res</span>(m,<span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n,<span class="number">0</span>));</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;m;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;n;j++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="type">int</span> sum=<span class="number">0</span>;</span><br><span class="line">                <span class="type">int</span> cnt=<span class="number">0</span>;</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> k=<span class="number">0</span>;k&lt;<span class="number">9</span>;k++)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="type">int</span> tx=i+dx[k];</span><br><span class="line">                    <span class="type">int</span> ty=j+dy[k];</span><br><span class="line">                    <span class="keyword">if</span>(tx&gt;=<span class="number">0</span>&amp;&amp;tx&lt;m&amp;&amp;ty&gt;=<span class="number">0</span>&amp;&amp;ty&lt;n) <span class="comment">//表示可以算到平均值当中的</span></span><br><span class="line">                    &#123;</span><br><span class="line">                        sum+=img[tx][ty];</span><br><span class="line">                        cnt++;</span><br><span class="line">                    &#125;  </span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">//cout&lt;&lt;sum&lt;&lt;cnt&lt;&lt;endl;</span></span><br><span class="line">                res[i][j]=(sum/cnt);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="7-leetcode-350-两个数组的交集Ⅱ"><a href="#7-leetcode-350-两个数组的交集Ⅱ" class="headerlink" title="7.leetcode 350 两个数组的交集Ⅱ"></a>7.leetcode 350 两个数组的交集Ⅱ</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/intersection-of-two-arrays-ii/">350. 两个数组的交集 II - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<ul>
<li>(1)纯哈希做法(两个数组,然后比对)</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">intersect</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums1, vector&lt;<span class="type">int</span>&gt;&amp; nums2)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//使用哈希表</span></span><br><span class="line">        <span class="type">int</span> map1[<span class="number">1010</span>]=&#123;<span class="number">0</span>&#125;;</span><br><span class="line">        <span class="type">int</span> map2[<span class="number">1010</span>]=&#123;<span class="number">0</span>&#125;;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;nums1.<span class="built_in">size</span>();i++)</span><br><span class="line">        &#123;</span><br><span class="line">            map1[nums1[i]]++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;nums2.<span class="built_in">size</span>();i++)</span><br><span class="line">        &#123;</span><br><span class="line">            map2[nums2[i]]++;</span><br><span class="line">        &#125;</span><br><span class="line">        vector&lt;<span class="type">int</span>&gt; res;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;<span class="number">1010</span>;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> cnt=<span class="built_in">min</span>(map1[i],map2[i]);</span><br><span class="line">            <span class="keyword">if</span>(cnt&gt;<span class="number">0</span>)</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;cnt;j++)</span><br><span class="line">                    res.<span class="built_in">push_back</span>(i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="8-leetcode-349-两个数组的交集"><a href="#8-leetcode-349-两个数组的交集" class="headerlink" title="8.leetcode 349 两个数组的交集"></a>8.leetcode 349 两个数组的交集</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/intersection-of-two-arrays/submissions/">349. 两个数组的交集 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">intersection</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; nums1, vector&lt;<span class="type">int</span>&gt;&amp; nums2)</span> </span>&#123;</span><br><span class="line">        vector&lt;<span class="type">int</span>&gt; res;</span><br><span class="line">        <span class="type">int</span> num1[<span class="number">1010</span>]=&#123;<span class="number">0</span>&#125;;</span><br><span class="line">        <span class="type">int</span> num2[<span class="number">1010</span>]=&#123;<span class="number">0</span>&#125;;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;nums1.<span class="built_in">size</span>();i++)</span><br><span class="line">            num1[nums1[i]]++;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;nums2.<span class="built_in">size</span>();i++)</span><br><span class="line">            num2[nums2[i]]++;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;<span class="number">1010</span>;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> cnt=<span class="built_in">min</span>(num1[i],num2[i]);</span><br><span class="line">            <span class="keyword">if</span>(cnt&gt;<span class="number">0</span>)</span><br><span class="line">                res.<span class="built_in">emplace_back</span>(i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="9-leetcode-434-字符串中的单词数"><a href="#9-leetcode-434-字符串中的单词数" class="headerlink" title="9.leetcode 434 字符串中的单词数"></a>9.leetcode 434 字符串中的单词数</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">countSegments</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> cnt=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(s.<span class="built_in">empty</span>()) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;s.<span class="built_in">size</span>()<span class="number">-1</span>;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">//什么样的算一个单词?</span></span><br><span class="line">            <span class="keyword">if</span>(s[i]!=<span class="string">&#x27; &#x27;</span>&amp;&amp;s[i+<span class="number">1</span>]==<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                cnt++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(s[s.<span class="built_in">size</span>()<span class="number">-1</span>]!=<span class="string">&#x27; &#x27;</span>) cnt++;</span><br><span class="line">        <span class="keyword">return</span> cnt;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="10-leetcode-1732-找到最高海拔"><a href="#10-leetcode-1732-找到最高海拔" class="headerlink" title="10.leetcode 1732 找到最高海拔"></a>10.leetcode 1732 找到最高海拔</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/find-the-highest-altitude/">1732. 找到最高海拔 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<ul>
<li>解法1:直接遍历数组,然后求和求解,注意数组下标的问题</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">largestAltitude</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; gain)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> maxres=<span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> temp=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=gain.<span class="built_in">size</span>();i++)</span><br><span class="line">        &#123;</span><br><span class="line">            temp+=gain[i<span class="number">-1</span>];</span><br><span class="line">            maxres=<span class="built_in">max</span>(maxres,temp);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> maxres;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="11-leetcode-344-反转字符串"><a href="#11-leetcode-344-反转字符串" class="headerlink" title="11.leetcode 344 反转字符串"></a>11.leetcode 344 反转字符串</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/reverse-string/">344. 反转字符串 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">reverseString</span><span class="params">(vector&lt;<span class="type">char</span>&gt;&amp; s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> l=<span class="number">0</span>,r=s.<span class="built_in">size</span>()<span class="number">-1</span>;l&lt;=r;) <span class="built_in">swap</span>(s[l++],s[r--]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>(水题)</p>
<h4 id="12-leetcode-206-反转链表"><a href="#12-leetcode-206-反转链表" class="headerlink" title="12.leetcode 206 反转链表"></a>12.leetcode 206 反转链表</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/reverse-linked-list/">206. 反转链表 - 力扣（LeetCode） (leetcode-cn.com)</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"> * struct ListNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     ListNode *next;</span></span><br><span class="line"><span class="comment"> *     ListNode() : val(0), next(nullptr) &#123;&#125;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x) : val(x), next(nullptr) &#123;&#125;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x, ListNode *next) : val(x), next(next) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">reverseList</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//反转链表的题目</span></span><br><span class="line">        <span class="comment">//来个虚拟头</span></span><br><span class="line">        <span class="keyword">if</span>(head==<span class="literal">nullptr</span>) <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">        ListNode* pre=<span class="literal">nullptr</span>;</span><br><span class="line">        ListNode *cur=head;</span><br><span class="line">        ListNode *nextnode=head;</span><br><span class="line">        <span class="keyword">while</span>(cur)</span><br><span class="line">        &#123;</span><br><span class="line">            nextnode=cur-&gt;next;</span><br><span class="line">            cur-&gt;next=pre;</span><br><span class="line">            pre=cur;</span><br><span class="line">            cur=nextnode;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> pre;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<h4 id="13-leetcode-19-删除链表的倒数第N个结点"><a href="#13-leetcode-19-删除链表的倒数第N个结点" class="headerlink" title="13.leetcode 19 删除链表的倒数第N个结点"></a>13.leetcode 19 删除链表的倒数第N个结点</h4><p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/remove-nth-node-from-end-of-list/">19. 删除链表的倒数第 N 个结点 - 力扣（LeetCode） (leetcode-cn.com)</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="ChrisZhang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">ChrisZhang</p>
  <div class="site-description" itemprop="description">我的技美学习之路</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hhlovesyy" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hhlovesyy" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1596944152@qq.com" title="E-Mail → mailto:1596944152@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/liang-dao-men" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;liang-dao-men" rel="noopener" target="_blank"><i class="fa fa-quora fa-fw"></i>知乎</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      链接网站
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.zhihu.com/people/liang-dao-men" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;liang-dao-men" rel="noopener" target="_blank">个人知乎</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://space.bilibili.com/24361666" title="https:&#x2F;&#x2F;space.bilibili.com&#x2F;24361666" rel="noopener" target="_blank">个人bilibili</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; Sat Apr 30 2022 08:00:00 GMT+0800 (中国标准时间) – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ChrisZhang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>

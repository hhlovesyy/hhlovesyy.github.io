<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="TensorFlow  学习(1)参考视频: 【北京大学】Tensorflow2.0_哔哩哔哩_bilibili 一.初识神经网络1.1 人工智能三学派 行为主义 基于控制论,构建感知-动作控制系统   符号主义 基于算数逻辑表达式,可用公式描述(如专家系统)   连接主义 仿生学,模仿神经元连接关系     1.2 神经网络设计过程案例:给鸢尾花分类(基于神经网络的方法，具体过程见[鸢尾花分类]">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow基础学习(1)">
<meta property="og:url" content="http://example.com/2022/05/02/Tensorflow%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0-1/index.html">
<meta property="og:site_name" content="ChrisZhang">
<meta property="og:description" content="TensorFlow  学习(1)参考视频: 【北京大学】Tensorflow2.0_哔哩哔哩_bilibili 一.初识神经网络1.1 人工智能三学派 行为主义 基于控制论,构建感知-动作控制系统   符号主义 基于算数逻辑表达式,可用公式描述(如专家系统)   连接主义 仿生学,模仿神经元连接关系     1.2 神经网络设计过程案例:给鸢尾花分类(基于神经网络的方法，具体过程见[鸢尾花分类]">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501210406652.png">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501210852707.png">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501211244083.png">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501212540892.png">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501212810950.png">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501214431416.png">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502104026064.png">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502110601376.png">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502111159869.png">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502111901887.png">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502113123185.png">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502115457987.png">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502115605657.png">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502162605534.png">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502162245222.png">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502162326718.png">
<meta property="og:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502162402489.png">
<meta property="article:published_time" content="2022-05-02T08:34:57.000Z">
<meta property="article:modified_time" content="2022-05-21T07:55:26.777Z">
<meta property="article:author" content="ChrisZhang">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="tensorflow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501210406652.png">

<link rel="canonical" href="http://example.com/2022/05/02/Tensorflow%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0-1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Tensorflow基础学习(1) | ChrisZhang</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">ChrisZhang</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-download fa-fw"></i>资源</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/02/Tensorflow%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="ChrisZhang">
      <meta itemprop="description" content="我的技美学习之路">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ChrisZhang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Tensorflow基础学习(1)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-05-02 16:34:57" itemprop="dateCreated datePublished" datetime="2022-05-02T16:34:57+08:00">2022-05-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-05-21 15:55:26" itemprop="dateModified" datetime="2022-05-21T15:55:26+08:00">2022-05-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/tensorflow/" itemprop="url" rel="index"><span itemprop="name">tensorflow</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="TensorFlow-学习-1"><a href="#TensorFlow-学习-1" class="headerlink" title="TensorFlow  学习(1)"></a>TensorFlow  学习(1)</h1><p>参考视频:</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1B7411L7Qt?p=1">【北京大学】Tensorflow2.0_哔哩哔哩_bilibili</a></p>
<h1 id="一-初识神经网络"><a href="#一-初识神经网络" class="headerlink" title="一.初识神经网络"></a>一.初识神经网络</h1><h2 id="1-1-人工智能三学派"><a href="#1-1-人工智能三学派" class="headerlink" title="1.1 人工智能三学派"></a>1.1 人工智能三学派</h2><ul>
<li>行为主义<ul>
<li>基于控制论,构建感知-动作控制系统</li>
</ul>
</li>
<li>符号主义<ul>
<li>基于算数逻辑表达式,可用公式描述(如专家系统)</li>
</ul>
</li>
<li>连接主义<ul>
<li>仿生学,<strong>模仿神经元连接关系</strong></li>
</ul>
</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501210406652.png" alt="image-20220501210406652"></p>
<h2 id="1-2-神经网络设计过程"><a href="#1-2-神经网络设计过程" class="headerlink" title="1.2 神经网络设计过程"></a>1.2 神经网络设计过程</h2><p>案例:给鸢尾花分类(基于神经网络的方法，具体过程见[鸢尾花分类](# 1.6 鸢尾花分类——数据集读入))</p>
<ul>
<li><strong>神经网络:<strong>采集大量输入特征,以及对应的类别(作为</strong>标签,需要人工标定</strong>)作为数据对构成数据集.</li>
<li>将数据集喂入搭建好的神经网络结构,<strong>网络优化参数得到模型</strong>,模型读入新输入的特征,然后输出识别的结果.</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501210852707.png" alt="image-20220501210852707"></p>
<p><strong>注意,w和b会被随机赋予一些初值,然后根据前向传播和反向传播来调整参数</strong></p>
<ul>
<li>利用<strong>损失函数</strong>来定义预测值$y$与标准答案$y’$的差距<ul>
<li>常用的损失函数有<strong>均方误差</strong>等</li>
</ul>
</li>
</ul>
<p><img src="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501211244083.png" alt="image-20220501211244083"></p>
<h3 id="1-尝试一个反向传播使梯度减小的例子"><a href="#1-尝试一个反向传播使梯度减小的例子" class="headerlink" title="(1)尝试一个反向传播使梯度减小的例子"></a>(1)尝试一个反向传播使梯度减小的例子</h3><ul>
<li>注：<strong>如果下例中有难以理解的函数，或许看完后面部分再回来就会更好理解</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w = tf.Variable(tf.constant(<span class="number">5</span>, dtype=tf.float32))  <span class="comment"># 设定参数w的随机初始值为5,可训练</span></span><br><span class="line">lr = <span class="number">0.2</span>  <span class="comment"># 学习率</span></span><br><span class="line">epoch = <span class="number">40</span>  <span class="comment"># 循环迭代次数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):  <span class="comment"># 对数据集循环epoch次</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># with结构到grads框起了梯度的计算过程</span></span><br><span class="line">        loss = tf.square(w + <span class="number">1</span>)  <span class="comment"># 损失函数</span></span><br><span class="line">    grads = tape.gradient(loss, w)  <span class="comment"># .gradient函数告知谁对谁求导</span></span><br><span class="line"></span><br><span class="line">    w.assign_sub(lr * grads)  <span class="comment"># .assign_sub 对变量做自减,即w-=lr*grads</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;After %s epoch,w is %f,loss is %f&quot;</span> % (epoch, w.numpy(), loss))</span><br><span class="line"></span><br><span class="line"><span class="comment"># lr初始值:0.2 请自选学习率0.001,0.009查看收敛过程</span></span><br><span class="line"><span class="comment"># 最终目的:找到loss最小 即w=-1的最优参数w</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="学习率过小或过大的情况"><a href="#学习率过小或过大的情况" class="headerlink" title="学习率过小或过大的情况"></a>学习率过小或过大的情况</h4><p>修改学习率lr的值,测试学习率过小或者过大的结果:</p>
<p><img src="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501212540892.png" alt="image-20220501212540892"></p>
<h2 id="1-3-张量生成"><a href="#1-3-张量生成" class="headerlink" title="1.3 张量生成"></a>1.3 张量生成</h2><p>Tensor的意思就是张量,可以直观理解成<strong>多维数组(列表)</strong>,其中张量的维数叫做<strong>阶</strong></p>
<p><img src="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501212810950.png" alt="image-20220501212810950"></p>
<p><code>[</code>的层数可以很好地说明张量的阶数。</p>
<p>其他的数据类型如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.int32,tf.float32,t.constant([True,False]),tf.constant(&quot;Hello,world!&quot;)等</span><br></pre></td></tr></table></figure>



<h3 id="（1）如何创建一个张量？"><a href="#（1）如何创建一个张量？" class="headerlink" title="（1）如何创建一个张量？"></a>（1）如何创建一个张量？</h3><p>使用constant创建一个张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">tf.constant([<span class="number">1</span>,<span class="number">5</span>],dtype=tf.int64)  <span class="comment"># 一阶张量</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.dtype)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br></pre></td></tr></table></figure>

<p>创建张量模板：</p>
<p><code>tf.constant(张量内容,dtype=数据类型（可选）)</code></p>
<ul>
<li>直接打印a会输出张量的所有信息，其中shape的逗号隔开了几个数字，张量就是几维的（比如上例shape隔开了一个数字，就是一维的，2表示张量里有2个元素）</li>
<li>也可以打印<code>a.dtype</code> 和<code>a.shape</code>，如上。</li>
</ul>
<h3 id="（2）Numpy数据转为Tensor"><a href="#（2）Numpy数据转为Tensor" class="headerlink" title="（2）Numpy数据转为Tensor"></a>（2）Numpy数据转为Tensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.arange(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">b = tf.convert_to_tensor(a, dtype=tf.int64)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果：</span></span><br><span class="line">[<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span>]</span><br><span class="line">tf.Tensor([<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span>], shape=(<span class="number">5</span>,), dtype=int64)</span><br></pre></td></tr></table></figure>



<h3 id="（3）创建一些固定的张量"><a href="#（3）创建一些固定的张量" class="headerlink" title="（3）创建一些固定的张量"></a>（3）创建一些固定的张量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.zeros([<span class="number">2</span>, <span class="number">3</span>])  <span class="comment"># 参数：维度</span></span><br><span class="line">b = tf.ones([<span class="number">4</span>])</span><br><span class="line">c = tf.fill([<span class="number">2</span>, <span class="number">2</span>], <span class="number">9</span>)  <span class="comment"># 参数:维度,指定值</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">tf.Tensor([[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>][<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]], shape=(<span class="number">2</span>, <span class="number">3</span>),dtype=float32)</span><br><span class="line">tf.Tensor([<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>], shape=(<span class="number">4</span>,),dtype=float32)</span><br><span class="line">tf.Tensor([[<span class="number">9</span> <span class="number">9</span>][<span class="number">9</span> <span class="number">9</span>]], shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=int32)</span><br></pre></td></tr></table></figure>



<h3 id="（4）随机生成随机数"><a href="#（4）随机生成随机数" class="headerlink" title="（4）随机生成随机数"></a>（4）随机生成随机数</h3><ul>
<li>生成正态分布随机数，默认均值0，标准差1</li>
</ul>
<p><code>tf.random.normal(维度,mean=均值,stddev=标准差)</code></p>
<ul>
<li>生成截断式正态分布的随机数(<strong>使得生成的随机数可以更集中一些</strong>)</li>
</ul>
<p><code>tf.random_truncated_normal(维度,mean=均值,stddev=标准差)</code></p>
<p>对截断式正态分布进行补充：</p>
<p><img src="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220501214431416.png" alt="image-20220501214431416"></p>
<p>示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">d = tf.random.normal([<span class="number">2</span>, <span class="number">2</span>], mean=<span class="number">0.5</span>, stddev=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"></span><br><span class="line">e = tf.random.truncated_normal([<span class="number">2</span>, <span class="number">2</span>], mean=<span class="number">0.5</span>, stddev=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(e)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">tf.Tensor([[<span class="number">0.9194691</span> <span class="number">0.8282855</span>] [<span class="number">2.7526226</span> <span class="number">0.9153044</span>]], shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32)</span><br><span class="line">tf.Tensor([[-<span class="number">0.77154016</span>  <span class="number">0.6888927</span> ] [ <span class="number">1.4981992</span>   <span class="number">1.229878</span>  ]], shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32)</span><br></pre></td></tr></table></figure>



<h3 id="（5）生成均匀分布随机数"><a href="#（5）生成均匀分布随机数" class="headerlink" title="（5）生成均匀分布随机数"></a>（5）生成均匀分布随机数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.random.uniform(维度,minval=最小值,maxval=最大值)</span><br></pre></td></tr></table></figure>

<p>比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f=tf.random.uniform([<span class="number">2</span>,<span class="number">2</span>],minval=<span class="number">0</span>,maxval=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行结果：</span></span><br><span class="line">tf.Tensor([[<span class="number">0.46490908</span> <span class="number">0.5134711</span> ][<span class="number">0.71371794</span> <span class="number">0.76942956</span>]], shape=(<span class="number">2</span>, <span class="number">2</span>), dtype=float32)</span><br></pre></td></tr></table></figure>



<h2 id="1-4-1-5-Tensorflow常用函数"><a href="#1-4-1-5-Tensorflow常用函数" class="headerlink" title="1.4-1.5 Tensorflow常用函数"></a>1.4-1.5 Tensorflow常用函数</h2><p>注：以下tensor_name表示张量的名字</p>
<p>（1）强制tensor转换为该数据类型</p>
<p><code>tf.cast(tensor_name,dtype=xxx)</code></p>
<p>（2）计算张量维度上元素的最小值</p>
<p><code>tf.reduce_min(tensor_name)</code></p>
<p>（3）计算张量维度上元素的最大值</p>
<p><code>tf.reduce_max(tensor_name)</code></p>
<p>一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x1 = tf.constant([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], dtype=tf.float64)</span><br><span class="line"><span class="built_in">print</span>(x1)</span><br><span class="line"></span><br><span class="line">x2 = tf.cast(x1, tf.int32)</span><br><span class="line"><span class="built_in">print</span>(x2)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tf.reduce_min(x2), tf.reduce_max(x2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">tf.Tensor([<span class="number">1.</span> <span class="number">2.</span> <span class="number">3.</span>], shape=(<span class="number">3</span>,), dtype=float64)</span><br><span class="line">tf.Tensor([<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>], shape=(<span class="number">3</span>,), dtype=int32)</span><br><span class="line">tf.Tensor(<span class="number">1</span>, shape=(), dtype=int32) tf.Tensor(<span class="number">3</span>, shape=(), dtype=int32)</span><br></pre></td></tr></table></figure>



<p>（4）<strong>axis</strong>函数</p>
<p>在一个二维张量或数组中，可以通过调整axis&#x3D;0或1来控制执行维度，（<strong>axis&#x3D;0表示跨行（经度，down），axis&#x3D;1表示跨列（纬度，across）</strong>）</p>
<p>如果不指定axis，则所有元素参与运算。</p>
<p><img src="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502104026064.png" alt="image-20220502104026064"></p>
<p>（5）计算张量沿着指定维度的平均值</p>
<p><code>tf.reduce_mean(tensor_name,axis=XXX)</code></p>
<p>（6）计算张量沿着指定维度的和</p>
<p><code>tf.reduce_sum(tensor_name,axis=XXX)</code></p>
<p>一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>]])  <span class="comment"># 两维的</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tf.reduce_mean(x))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tf.reduce_sum(x, axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行结果</span></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">2</span> <span class="number">3</span>]], shape=(<span class="number">2</span>, <span class="number">3</span>), dtype=int32)</span><br><span class="line"></span><br><span class="line">tf.Tensor(<span class="number">2</span>, shape=(), dtype=int32)</span><br><span class="line">tf.Tensor([<span class="number">6</span> <span class="number">7</span>], shape=(<span class="number">2</span>,), dtype=int32)  <span class="comment"># 沿着横向移动，所以每行求个和</span></span><br></pre></td></tr></table></figure>



<h3 id="（7）标记可训练"><a href="#（7）标记可训练" class="headerlink" title="（7）标记可训练"></a><strong>（7）标记可训练</strong></h3><p>被标记的变量会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记待训练参数。</p>
<p><code>tf.Variable(初始值)</code></p>
<p>例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w=tf.Variable(tf.random.normal[<span class="number">2</span>,<span class="number">2</span>],mean=<span class="number">0</span>,stddev=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>



<p>（8）常用数学运算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.add(tensor1,tensor2)</span><br><span class="line">tf.subtract(tensor1,tensor2)</span><br><span class="line">tf.multiply(tensor1,tensor2)</span><br><span class="line">tf.divide(tensor1,tensor2)</span><br><span class="line"></span><br><span class="line">tf.square(tensor1)</span><br><span class="line">tf.<span class="built_in">pow</span>(tensor1,n)</span><br><span class="line">tf,sqrt(tensor1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵乘法</span></span><br><span class="line">tf.matmul(matrix1,matrix2)</span><br></pre></td></tr></table></figure>

<p><strong>只有维度相同的张量才能进行四则运算。</strong></p>
<h3 id="（9）特征配对函数"><a href="#（9）特征配对函数" class="headerlink" title="（9）特征配对函数"></a>（9）特征配对函数</h3><p><code>tf.data.Dateset.from_tensor_slices((输入特征，标签))</code></p>
<p><strong>切分传入张量的第一维度，生成输入特征标签对，构建数据集。</strong></p>
<p>注：Numpy和Tensor格式都可以用该语句读入数据</p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">features = tf.constant([<span class="number">12</span>, <span class="number">23</span>, <span class="number">10</span>, <span class="number">17</span>])</span><br><span class="line">labels = tf.constant([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices((features, labels))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dataset)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> element <span class="keyword">in</span> dataset:</span><br><span class="line">    <span class="built_in">print</span>(element)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 输出结果如下：</span></span><br><span class="line">(&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">9</span>, shape=(), dtype=int32, numpy=<span class="number">12</span>&gt;, &lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">10</span>, shape=(), dtype=int32, numpy=<span class="number">0</span>&gt;)</span><br><span class="line">(&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">11</span>, shape=(), dtype=int32, numpy=<span class="number">23</span>&gt;, &lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">12</span>, shape=(), dtype=int32, numpy=<span class="number">1</span>&gt;)</span><br><span class="line">(&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">13</span>, shape=(), dtype=int32, numpy=<span class="number">10</span>&gt;, &lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">14</span>, shape=(), dtype=int32, numpy=<span class="number">1</span>&gt;)</span><br><span class="line">(&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">15</span>, shape=(), dtype=int32, numpy=<span class="number">17</span>&gt;, &lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">16</span>, shape=(), dtype=int32, numpy=<span class="number">0</span>&gt;)</span><br></pre></td></tr></table></figure>

<p>可以看到，数据与标签进行了很好地匹配，这就是我们后续计算的基础。</p>
<h3 id="（10）实现函数对某参数的求导运算"><a href="#（10）实现函数对某参数的求导运算" class="headerlink" title="（10）实现函数对某参数的求导运算"></a>（10）实现函数对某参数的求导运算</h3><p><code>tf.GradientTape</code></p>
<ul>
<li>with结构记录计算过程，gradient求出张量的梯度</li>
</ul>
<p>结构如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    若干个计算过程</span><br><span class="line">grad=tape.gradient(函数,对谁求导)</span><br></pre></td></tr></table></figure>

<p>比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    w = tf.Variable(tf.constant(<span class="number">3.0</span>))</span><br><span class="line">    loss = tf.<span class="built_in">pow</span>(w, <span class="number">2</span>)</span><br><span class="line">grad = tape.gradient(loss, w)</span><br><span class="line"><span class="built_in">print</span>(grad)</span><br></pre></td></tr></table></figure>

<p><img src="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502110601376.png" alt="image-20220502110601376"></p>
<h3 id="（11）enumerate枚举"><a href="#（11）enumerate枚举" class="headerlink" title="（11）enumerate枚举"></a>（11）enumerate枚举</h3><p>这个函数是python的内建函数，可以遍历每个元素（如列表，元组或字符串），并组合为<strong>索引  元素</strong>，常在for循环中使用，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">seq = [<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;three&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i, element <span class="keyword">in</span> <span class="built_in">enumerate</span>(seq):</span><br><span class="line">    <span class="built_in">print</span>(i, element)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="number">0</span> one</span><br><span class="line"><span class="number">1</span> two</span><br><span class="line"><span class="number">2</span> three</span><br></pre></td></tr></table></figure>



<h3 id="（12）分类问题常用——独热编码"><a href="#（12）分类问题常用——独热编码" class="headerlink" title="（12）分类问题常用——独热编码"></a>（12）分类问题常用——独热编码</h3><p><code>tf.one_hot</code></p>
<p><strong>独热编码（one-hot encoding）：在分类问题中，常用独热码做标签，标记类别：1表示是，0表示非。</strong></p>
<p>比如：</p>
<p><img src="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502111159869.png" alt="image-20220502111159869"></p>
<p>（标签是1（杂色鸢尾），百分之0是狗尾草鸢尾，百分之0是弗吉尼亚鸢尾，百分之百是杂色鸢尾）</p>
<ul>
<li><code>tf.one_hot()</code>函数将待转换数据，转换为one_hot形式的数据输出.</li>
</ul>
<p><code>tf.one_hot(待转换数据,depth=几分类)</code></p>
<p>比如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">classes = <span class="number">3</span>  <span class="comment"># 三分类问题</span></span><br><span class="line">labels = tf.constant([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])  <span class="comment"># 输入元素最小值为0,最大值为2</span></span><br><span class="line">output = tf.one_hot(labels, depth=classes)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果:</span></span><br><span class="line">tf.Tensor(</span><br><span class="line">[[<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]], shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=float32)</span><br></pre></td></tr></table></figure>



<h3 id="（13）tf-nn-softmax"><a href="#（13）tf-nn-softmax" class="headerlink" title="（13）tf.nn.softmax"></a>（13）tf.nn.softmax</h3><p>对于分类问题，神经网络完成前向传播，计算出了每种类型的可能性大小，<strong>但这些可能性输出需要符合概率分布，如下述示意图：</strong></p>
<p><img src="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502111901887.png" alt="image-20220502111901887"></p>
<p>以图中为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">y = tf.constant([<span class="number">1.01</span>, <span class="number">2.01</span>, -<span class="number">0.66</span>])</span><br><span class="line">y_pro = tf.nn.softmax(y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;After softmax,y_pro is:&quot;</span>, y_pro)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果：</span></span><br><span class="line">After softmax,y_pro <span class="keyword">is</span>: tf.Tensor([<span class="number">0.25598174</span> <span class="number">0.69583046</span> <span class="number">0.0481878</span> ], shape=(<span class="number">3</span>,), dtype=float32)</span><br></pre></td></tr></table></figure>



<h3 id="（14）自减函数"><a href="#（14）自减函数" class="headerlink" title="（14）自减函数"></a>（14）自减函数</h3><ul>
<li>该函数要求数据是可训练的（需要用tf.Variable先定义可训练）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w=tf.Variable(<span class="number">4</span>)</span><br><span class="line">w.assign_sub(<span class="number">1</span>)  <span class="comment"># 即w-=1</span></span><br><span class="line"><span class="built_in">print</span>(w)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">&lt;tf.Variable <span class="string">&#x27;Variable:0&#x27;</span> shape=() dtype=int32, numpy=<span class="number">3</span>&gt;</span><br></pre></td></tr></table></figure>



<h3 id="（15）返回张量沿指定维度最大值的索引"><a href="#（15）返回张量沿指定维度最大值的索引" class="headerlink" title="（15）返回张量沿指定维度最大值的索引"></a>（15）返回张量沿指定维度最大值的索引</h3><p><code>tf.argmax(tensor_name,axis=XXX)</code></p>
<p>比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">test = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">8</span>, <span class="number">7</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="built_in">print</span>(test)</span><br><span class="line"><span class="built_in">print</span>(tf.argmax(test, axis=<span class="number">0</span>))  <span class="comment"># 返回每一列(经度)的最大值的索引</span></span><br><span class="line"><span class="built_in">print</span>(tf.argmax(test, axis=<span class="number">1</span>))  <span class="comment"># 返回每一行(纬度)的最大值的索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果：</span></span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">3</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">4</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">8</span> <span class="number">7</span> <span class="number">2</span>]]</span><br><span class="line">tf.Tensor([<span class="number">3</span> <span class="number">3</span> <span class="number">1</span>], shape=(<span class="number">3</span>,), dtype=int64)</span><br><span class="line">tf.Tensor([<span class="number">2</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span>], shape=(<span class="number">4</span>,), dtype=int64)</span><br></pre></td></tr></table></figure>



<h2 id="1-6-鸢尾花分类——数据集读入"><a href="#1-6-鸢尾花分类——数据集读入" class="headerlink" title="1.6 鸢尾花分类——数据集读入"></a>1.6 鸢尾花分类——数据集读入</h2><p>数据集介绍：</p>
<p><img src="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502113123185.png" alt="image-20220502113123185"></p>
<p><strong>这个用例需要用到Pandas包和sklearn包，可以通过pip install来进行安装（有可能Pycharm无法找到sklearn，所以用pip install会比较稳妥），具体可以看[Tensorflow安装]这一篇博客</strong></p>
<p>可以用sklearn包datasets读入数据集，语法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">x_data = datasets.load_iris().data</span><br><span class="line">y_data = datasets.load_iris().target</span><br></pre></td></tr></table></figure>

<p>完整的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">x_data = datasets.load_iris().data</span><br><span class="line">y_data = datasets.load_iris().target</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_data from datasets:\n&quot;</span>, x_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_data from datasets:\n&quot;</span>, y_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了美观,采用表格的方式输出</span></span><br><span class="line">x_data = DataFrame(x_data, columns=[<span class="string">&#x27;花萼长度&#x27;</span>, <span class="string">&#x27;花萼宽度&#x27;</span>, <span class="string">&#x27;花瓣长度&#x27;</span>, <span class="string">&#x27;花瓣宽度&#x27;</span>])</span><br><span class="line">pd.set_option(<span class="string">&#x27;display.unicode.east_asian_width&#x27;</span>, <span class="literal">True</span>)  <span class="comment"># 设置列名对齐</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_data add index: \n&quot;</span>, x_data)</span><br><span class="line"></span><br><span class="line">x_data[<span class="string">&#x27;类别&#x27;</span>] = y_data  <span class="comment"># 新加一列,列标签为&quot;类别&quot;,数据为y_data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_data add a column:\n&quot;</span>, x_data)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>最后可以看到的数据集+标签效果如下：</p>
<p><img src="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502115457987.png" alt="image-20220502115457987"></p>
<h2 id="1-7-神经网络实现鸢尾花分类-重要！"><a href="#1-7-神经网络实现鸢尾花分类-重要！" class="headerlink" title="1.7 神经网络实现鸢尾花分类(重要！)"></a>1.7 神经网络实现鸢尾花分类(重要！)</h2><p>具体的步骤如下：</p>
<p><img src="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502115605657.png" alt="image-20220502115605657"></p>
<p>整体过程如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.数据集读入(一共150组)</span></span><br><span class="line">x_data = datasets.load_iris().data</span><br><span class="line">y_data = datasets.load_iris().target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.数据集乱序(模拟人脑乱序处理信息)</span></span><br><span class="line">np.random.seed(<span class="number">116</span>)  <span class="comment"># 使用相同的seed,使输入特征/标签一一对应</span></span><br><span class="line">np.random.shuffle(x_data)</span><br><span class="line">np.random.seed(<span class="number">116</span>)</span><br><span class="line">np.random.shuffle(y_data)</span><br><span class="line">tf.random.set_seed(<span class="number">116</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.数据集分出永不相见的训练集和测试集(训练集和测试集不能重复)</span></span><br><span class="line">x_train = x_data[:-<span class="number">30</span>]  <span class="comment"># 前120个数据取出来作为训练集</span></span><br><span class="line">y_train = y_data[:-<span class="number">30</span>]</span><br><span class="line">x_test = x_data[-<span class="number">30</span>:]  <span class="comment"># 后30个数据取出来作为测试集</span></span><br><span class="line">y_test = y_data[-<span class="number">30</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换x的数据类型,否则后面矩阵相乘时会因数据类型不一致而报错</span></span><br><span class="line">x_train = tf.cast(x_train, tf.float32)</span><br><span class="line">x_test = tf.cast(x_test, tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.配成[输入特征,标签]对,每次喂入一个batch</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(<span class="number">32</span>)</span><br><span class="line">test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(<span class="number">32</span>)  <span class="comment"># 每32组打包成一个batch</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.定义神经网络的所有可训练参数,只有一层网络,输入结点四个,输出结点3个(3分类)</span></span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([<span class="number">4</span>, <span class="number">3</span>], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br><span class="line">b1 = tf.Variable(tf.random.truncated_normal([<span class="number">3</span>], stddev=<span class="number">0.1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一些超参数</span></span><br><span class="line">lr = <span class="number">0.1</span>  <span class="comment"># 学习率</span></span><br><span class="line"><span class="comment"># 下面两个是画图用的</span></span><br><span class="line">train_loss_results = []  <span class="comment"># 每轮的loss记录进去</span></span><br><span class="line">test_acc = []  <span class="comment"># 每轮的准确率记录进去</span></span><br><span class="line">epoch = <span class="number">500</span>  <span class="comment"># 迭代次数</span></span><br><span class="line">loss_all = <span class="number">0</span>  <span class="comment"># 每轮四个step(150数据量,32个一个batch,共四组batch训练一次,记录四个batch生成的4个loss的和)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.嵌套循环迭代,with结构更新参数,显示当前loss</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch):  <span class="comment"># 数据集级别迭代</span></span><br><span class="line">    <span class="keyword">for</span> step, (x_train, y_train) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db):  <span class="comment"># 针对batch级别的迭代</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># 记录梯度信息</span></span><br><span class="line">            y = tf.matmul(x_train, w1) + b1  <span class="comment"># 神经网络乘加运算:32*4 4*3,可以相乘</span></span><br><span class="line">            y = tf.nn.softmax(y)  <span class="comment"># 使得输出y符合概率分布(这样才能和独热码求均方误差loss)</span></span><br><span class="line">            y_ = tf.one_hot(y_train, depth=<span class="number">3</span>)  <span class="comment"># 转换为独热码模式方便求loss</span></span><br><span class="line">            loss = tf.reduce_mean(tf.square(y_ - y))  <span class="comment"># reduce_mean计算沿某一维度的平均值,默认所有</span></span><br><span class="line">            loss_all += loss.numpy()</span><br><span class="line">        <span class="comment"># 计算loss对各个参数的梯度</span></span><br><span class="line">        grads = tape.gradient(loss, [w1, b1])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 实现梯度下降</span></span><br><span class="line">        w1.assign_sub(lr * grads[<span class="number">0</span>])  <span class="comment"># 参数w1自更新,下面类似,每个batch训练完更新一次</span></span><br><span class="line">        b1.assign_sub(lr * grads[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个epoch,打印loss信息</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch&#123;&#125;,loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch, loss_all / <span class="number">4</span>))  <span class="comment"># 因为一次迭代是4个batch,所以这里/4</span></span><br><span class="line">    train_loss_results.append(loss_all / <span class="number">4</span>)</span><br><span class="line">    loss_all = <span class="number">0</span>  <span class="comment"># 归零,方便下次epoch的计算</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试部分,每次迭代都测试一下(主要是为了看学习过程)</span></span><br><span class="line">    <span class="comment"># total_correct为预测对的样本个数,total_number为测试的总样本数,都初始化为0</span></span><br><span class="line">    total_correct, total_number = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_test, y_test <span class="keyword">in</span> test_db:</span><br><span class="line">        <span class="comment"># 使用更新后的参数预测</span></span><br><span class="line">        y = tf.matmul(x_test, w1) + b1</span><br><span class="line">        y = tf.nn.softmax(y)</span><br><span class="line">        pred = tf.argmax(y, axis=<span class="number">1</span>)  <span class="comment"># 返回y中最大值的索引,即预测的分类</span></span><br><span class="line">        pred = tf.cast(pred, dtype=y_test.dtype)  <span class="comment"># 转换数据类型</span></span><br><span class="line">        <span class="comment"># 若分类正确,则correct=1,否则为0,将bool的结果转换为int型</span></span><br><span class="line">        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)</span><br><span class="line">        <span class="comment"># 将每个batch的correct数加起来</span></span><br><span class="line">        correct = tf.reduce_sum(correct)  <span class="comment"># reduce_sum计算张量沿着指定维度的和,默认全维度</span></span><br><span class="line">        <span class="comment"># 将所有batch中的correct数加起来</span></span><br><span class="line">        total_correct += <span class="built_in">int</span>(correct)</span><br><span class="line">        <span class="comment"># total_number为测试的总样本数,也就是x_test的行数</span></span><br><span class="line">        total_number += x_test.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 总的准确率计算</span></span><br><span class="line">    acc = total_correct / total_number</span><br><span class="line">    test_acc.append(acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test_acc:&quot;</span>, acc)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;--------------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制loss曲线</span></span><br><span class="line">plt.title(<span class="string">&#x27;Loss Function Curve&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.plot(train_loss_results, label=<span class="string">&quot;$Loss$&quot;</span>)  <span class="comment"># 逐点画出train_loss_results并连线.连线图标是Loss</span></span><br><span class="line">plt.legend()  <span class="comment"># 画出曲线坐标</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制Accuracy曲线</span></span><br><span class="line">plt.title(<span class="string">&#x27;Acc Curve&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Acc&#x27;</span>)</span><br><span class="line">plt.plot(test_acc, label=<span class="string">&quot;$Accuracy&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><strong>针对上述的代码，有几点注意事项：</strong></p>
<p>1、我们所搭建的神经网络如下：</p>
<p><img src="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502162605534.png"></p>
<p>（四层输入层，三层输出层，以全连接的形式）</p>
<p>2、注意所有的输入和输出（比如xtest，xtrain，ytest，ytrain）都是矩阵的形式，每个矩阵当中包含一整个batch的数据集，因此在做矩阵相乘的时候要注意维度是否匹配；</p>
<p>3、<strong>如果对于矩阵乘法仍感觉迷惑的话，需要复习一下与线性代数有关的内容，可以手动计算一下各个矩阵的维度和计算结果的维度</strong></p>
<p>最后的输出结果如下：</p>
<p>（1）首先是运行结果：可以看到，控制台输出的准确率越来越高，最后可以达到百分之百的准确率，同时损失函数值越来越小：</p>
<p><img src="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502162245222.png" alt="image-20220502162245222"></p>
<p>（2）观察损失函数值随迭代次数的变化，可以看到上述趋势：</p>
<p><img src="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502162326718.png"></p>
<p>（3）观察准确率随迭代次数的变化，可以看到正确的趋势：</p>
<p><img src="https://fastly.jsdelivr.net/gh/hhlovesyy/ImgHosting/TensorflowLearn/image-20220502162402489.png"></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>ChrisZhang
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2022/05/02/Tensorflow%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0-1/" title="Tensorflow基础学习(1)">http://example.com/2022/05/02/Tensorflow基础学习-1/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
              <a href="/tags/tensorflow/" rel="tag"># tensorflow</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/05/01/Tensorflow%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/" rel="prev" title="Tensorflow安装配置步骤">
      <i class="fa fa-chevron-left"></i> Tensorflow安装配置步骤
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/05/03/Tensowflow%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0-2/" rel="next" title="Tensowflow基础学习(2)">
      Tensowflow基础学习(2) <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorFlow-%E5%AD%A6%E4%B9%A0-1"><span class="nav-number">1.</span> <span class="nav-text">TensorFlow  学习(1)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%80-%E5%88%9D%E8%AF%86%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.</span> <span class="nav-text">一.初识神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%89%E5%AD%A6%E6%B4%BE"><span class="nav-number">2.1.</span> <span class="nav-text">1.1 人工智能三学派</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1%E8%BF%87%E7%A8%8B"><span class="nav-number">2.2.</span> <span class="nav-text">1.2 神经网络设计过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%B0%9D%E8%AF%95%E4%B8%80%E4%B8%AA%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BD%BF%E6%A2%AF%E5%BA%A6%E5%87%8F%E5%B0%8F%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-number">2.2.1.</span> <span class="nav-text">(1)尝试一个反向传播使梯度减小的例子</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BF%87%E5%B0%8F%E6%88%96%E8%BF%87%E5%A4%A7%E7%9A%84%E6%83%85%E5%86%B5"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">学习率过小或过大的情况</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-%E5%BC%A0%E9%87%8F%E7%94%9F%E6%88%90"><span class="nav-number">2.3.</span> <span class="nav-text">1.3 张量生成</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%881%EF%BC%89%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%BC%A0%E9%87%8F%EF%BC%9F"><span class="nav-number">2.3.1.</span> <span class="nav-text">（1）如何创建一个张量？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%882%EF%BC%89Numpy%E6%95%B0%E6%8D%AE%E8%BD%AC%E4%B8%BATensor"><span class="nav-number">2.3.2.</span> <span class="nav-text">（2）Numpy数据转为Tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%883%EF%BC%89%E5%88%9B%E5%BB%BA%E4%B8%80%E4%BA%9B%E5%9B%BA%E5%AE%9A%E7%9A%84%E5%BC%A0%E9%87%8F"><span class="nav-number">2.3.3.</span> <span class="nav-text">（3）创建一些固定的张量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%884%EF%BC%89%E9%9A%8F%E6%9C%BA%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0"><span class="nav-number">2.3.4.</span> <span class="nav-text">（4）随机生成随机数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%885%EF%BC%89%E7%94%9F%E6%88%90%E5%9D%87%E5%8C%80%E5%88%86%E5%B8%83%E9%9A%8F%E6%9C%BA%E6%95%B0"><span class="nav-number">2.3.5.</span> <span class="nav-text">（5）生成均匀分布随机数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-1-5-Tensorflow%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0"><span class="nav-number">2.4.</span> <span class="nav-text">1.4-1.5 Tensorflow常用函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%887%EF%BC%89%E6%A0%87%E8%AE%B0%E5%8F%AF%E8%AE%AD%E7%BB%83"><span class="nav-number">2.4.1.</span> <span class="nav-text">（7）标记可训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%889%EF%BC%89%E7%89%B9%E5%BE%81%E9%85%8D%E5%AF%B9%E5%87%BD%E6%95%B0"><span class="nav-number">2.4.2.</span> <span class="nav-text">（9）特征配对函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%8810%EF%BC%89%E5%AE%9E%E7%8E%B0%E5%87%BD%E6%95%B0%E5%AF%B9%E6%9F%90%E5%8F%82%E6%95%B0%E7%9A%84%E6%B1%82%E5%AF%BC%E8%BF%90%E7%AE%97"><span class="nav-number">2.4.3.</span> <span class="nav-text">（10）实现函数对某参数的求导运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%8811%EF%BC%89enumerate%E6%9E%9A%E4%B8%BE"><span class="nav-number">2.4.4.</span> <span class="nav-text">（11）enumerate枚举</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%8812%EF%BC%89%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E5%B8%B8%E7%94%A8%E2%80%94%E2%80%94%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81"><span class="nav-number">2.4.5.</span> <span class="nav-text">（12）分类问题常用——独热编码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%8813%EF%BC%89tf-nn-softmax"><span class="nav-number">2.4.6.</span> <span class="nav-text">（13）tf.nn.softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%8814%EF%BC%89%E8%87%AA%E5%87%8F%E5%87%BD%E6%95%B0"><span class="nav-number">2.4.7.</span> <span class="nav-text">（14）自减函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%8815%EF%BC%89%E8%BF%94%E5%9B%9E%E5%BC%A0%E9%87%8F%E6%B2%BF%E6%8C%87%E5%AE%9A%E7%BB%B4%E5%BA%A6%E6%9C%80%E5%A4%A7%E5%80%BC%E7%9A%84%E7%B4%A2%E5%BC%95"><span class="nav-number">2.4.8.</span> <span class="nav-text">（15）返回张量沿指定维度最大值的索引</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-6-%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%88%86%E7%B1%BB%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AF%BB%E5%85%A5"><span class="nav-number">2.5.</span> <span class="nav-text">1.6 鸢尾花分类——数据集读入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-7-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%88%86%E7%B1%BB-%E9%87%8D%E8%A6%81%EF%BC%81"><span class="nav-number">2.6.</span> <span class="nav-text">1.7 神经网络实现鸢尾花分类(重要！)</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="ChrisZhang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">ChrisZhang</p>
  <div class="site-description" itemprop="description">我的技美学习之路</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hhlovesyy" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hhlovesyy" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1596944152@qq.com" title="E-Mail → mailto:1596944152@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/liang-dao-men" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;liang-dao-men" rel="noopener" target="_blank"><i class="fa fa-quora fa-fw"></i>知乎</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      链接网站
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.zhihu.com/people/liang-dao-men" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;liang-dao-men" rel="noopener" target="_blank">个人知乎</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://space.bilibili.com/24361666" title="https:&#x2F;&#x2F;space.bilibili.com&#x2F;24361666" rel="noopener" target="_blank">个人bilibili</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; Sat Apr 30 2022 08:00:00 GMT+0800 (中国标准时间) – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ChrisZhang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
